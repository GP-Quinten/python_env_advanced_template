import logging
import time
import json
import random
from pathlib import Path
import glob
import click

logging.basicConfig(
    level=logging.WARNING,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


@click.command()
@click.option(
    "--input_jsonl_template",
    type=str,
    help="Input file template pattern where the original batch JSONL files are stored.",
)
@click.option(
    "--output_jsonl_template",
    type=str,
    help="Output file template where the new regrouped batches will be saved.",
)
@click.option(
    "--batch_limit",
    type=int,
    default=-1,
    help="Maximum number of input batches to process. Use -1 to process all available batches.",
)
@click.option(
    "--new_batch_size",
    type=int,
    default=2000,
    help="Number of publications to include in each new batch.",
)
def regroup_registry_related_publis_in_batches(
    input_jsonl_template: str,
    output_jsonl_template: str,
    batch_limit: int,
    new_batch_size: int,
):
    """
    Regroup registry-related publications from existing batches into new batches of uniform size.

    This script:
    1. Loads registry-related publications from batches generated by R02
    2. Shuffles them with a fixed random seed for consistency
    3. Creates new batches with uniform size
    4. Saves these new batches with sequential numbering
    """
    # Track execution time
    start_time = time.time()

    # Set a fixed random seed for consistency between runs
    random.seed(42)

    # Get input directory from template
    input_dir = Path(input_jsonl_template).parent

    # Get all input batch files and sort them by batch number
    batch_files = sorted(input_dir.glob("*.jsonl"), key=lambda p: int(p.stem))

    # Limit the number of batches if specified
    if batch_limit > 0:
        batch_files = batch_files[:batch_limit]
        logger.warning(f"Processing first {batch_limit} input batches")
    else:
        logger.warning(f"Processing all {len(batch_files)} input batches")

    # Create output directory
    output_dir = Path(output_jsonl_template).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load all registry-related publications from input batches
    all_registry_related_publications = []
    total_input_batches = len(batch_files)

    logger.warning(f"Loading publications from {total_input_batches} input batches...")

    for i, batch_file in enumerate(batch_files, 1):
        batch_num = batch_file.stem
        try:
            batch_publications = []
            with open(batch_file, "r", encoding="utf-8") as f:
                for line in f:
                    publication = json.loads(line)
                    batch_publications.append(publication)

            all_registry_related_publications.extend(batch_publications)
            logger.warning(
                f"Loaded {len(batch_publications)} publications from batch {batch_num} ({i}/{total_input_batches})"
            )
        except Exception as e:
            logger.error(f"Error loading batch {batch_num}: {str(e)}")

    total_publications = len(all_registry_related_publications)
    logger.warning(f"Loaded {total_publications} total registry-related publications")

    # Calculate number of new batches
    num_new_batches = (total_publications + new_batch_size - 1) // new_batch_size
    logger.warning(
        f"Creating {num_new_batches} new batches with {new_batch_size} publications each"
    )

    # Create and save new batches
    for batch_num in range(1, num_new_batches + 1):
        start_idx = (batch_num - 1) * new_batch_size
        end_idx = min(start_idx + new_batch_size, total_publications)

        batch_publications = all_registry_related_publications[start_idx:end_idx]

        # Format output file path
        output_file = output_dir / f"{batch_num}.jsonl"

        # Save batch to file
        with open(output_file, "w", encoding="utf-8") as f:
            for publication in batch_publications:
                f.write(json.dumps(publication) + "\n")

        logger.warning(
            f"Saved batch {batch_num}/{num_new_batches} with {len(batch_publications)} publications to {output_file}"
        )

    # Log execution statistics
    elapsed_time = time.time() - start_time
    logger.warning(f"Regrouping completed in {elapsed_time:.2f} seconds")
    logger.warning(f"Processed {total_input_batches} input batches")
    logger.warning(f"Created {num_new_batches} new batches")
    logger.warning(f"Total publications: {total_publications}")
    logger.warning(f"Publications per batch: {new_batch_size}")


if __name__ == "__main__":
    regroup_registry_related_publis_in_batches()
