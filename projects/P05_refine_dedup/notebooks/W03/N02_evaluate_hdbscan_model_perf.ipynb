{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304f9693",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4a68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.pyenv/versions/3.11.11/envs/P05_refine_dedup_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set working directory (adjust if needed)\n",
    "working_dir = '/home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup'\n",
    "os.chdir(working_dir)\n",
    "print(f'Changed working directory to {working_dir}')\n",
    "\n",
    "from src.p05_refine_dedup import config\n",
    "from src.p05_refine_dedup.utils.utils import (\n",
    "    is_noise,\n",
    "    run_hdbscan,\n",
    "    apply_predictions,\n",
    "    compute_metrics,\n",
    ")\n",
    "from src.p05_refine_dedup.utils.s3_io_functions import (\n",
    "    load_parquet_from_s3,\n",
    ")  # for loading embeddings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61214cda",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e9f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 14:35:36,943 INFO Loading embeddings from registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\n",
      "2025-08-09 14:35:36,944 WARNING Data will be loaded from s3://s3-common-dev20231214174437248800000002/registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "s3_input_embeddings = \"registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\"\n",
    "# 2. Load embeddings from S3 (parquet file)\n",
    "logger.info(f\"Loading embeddings from {s3_input_embeddings}\")\n",
    "bucket_name = config.BUCKET_NAME_DEV\n",
    "# folder_path = s3_input_embeddings, file_name = last part\n",
    "folder_path = s3_input_embeddings.rsplit(\"/\", 1)[0]\n",
    "file_name = s3_input_embeddings.rsplit(\"/\", 1)[-1]\n",
    "embeddings_df = load_parquet_from_s3(\n",
    "    bucket_name=bucket_name,\n",
    "    folder_path=folder_path,\n",
    "    file_name=file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5fc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset_any=\"data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/assessed_pairs_v1.xlsx\"\n",
    "evaluation_dataset_famous=\"data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/famous_close_assessed_pairs_v1.xlsx\"\n",
    "\n",
    "# best_config=\"etc/best_config.json\", # this is the best config obtained from the previous step, in json format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170c1887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load evaluation dataset (any pairs) from data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/assessed_pairs_v1.xlsx\n",
      "Load evaluation dataset (famous pairs) from data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/famous_close_assessed_pairs_v1.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(f\"Load evaluation dataset (any pairs) from {evaluation_dataset_any}\")\n",
    "eval_df_any = pd.read_excel(evaluation_dataset_any)\n",
    "print(f\"Load evaluation dataset (famous pairs) from {evaluation_dataset_famous}\")\n",
    "eval_df_famous = pd.read_excel(evaluation_dataset_famous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4dd21",
   "metadata": {},
   "source": [
    "# 1. Compute first global clustering with hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c601d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define outputs\n",
    "version=\"vtest\"\n",
    "step=1\n",
    "clusters_table_output = f\"data/W03/from_notebooks/R03_evaluate_hdbscan_model_performance/{version}/clusters_table.xlsx\"\n",
    "# make sure the output directory exists\n",
    "output_dir = Path(clusters_table_output).parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prediction_any_results_xlsx=f\"data/W03/from_notebooks/R03_evaluate_hdbscan_model_performance/{version}/prediction_results_any_reg.xlsx\"\n",
    "prediction_famous_results_xlsx=f\"data/W03/from_notebooks/R03_evaluate_hdbscan_model_performance/{version}/prediction_results_famous_reg.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c85994",
   "metadata": {},
   "source": [
    "## Apply HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ca9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selet top n rows for testing\n",
    "n_rows = 100\n",
    "embeddings_df = embeddings_df.head(n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b784fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=4\n",
    "min_samples=4\n",
    "cluster_selection_epsilon=0.0\n",
    "max_cluster_size=30\n",
    "metric=\"euclidean\"\n",
    "n_jobs=-1\n",
    "cluster_selection_method=\"leaf\"\n",
    "store_centers=\"medoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "249c1629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 14:36:49,730 INFO HDBSCAN completed in 0.04 seconds with min_cluster_size=4, min_samples=4.\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.vstack(embeddings_df[\"full_name_embedding\"].values)\n",
    "labels, comp_time = run_hdbscan(\n",
    "    embeddings,\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "    max_cluster_size=max_cluster_size,\n",
    "    metric=metric,\n",
    "    n_jobs=n_jobs,\n",
    "    cluster_selection_method=cluster_selection_method,\n",
    "    store_centers=store_centers,\n",
    ")\n",
    "clusters_df = embeddings_df.copy()\n",
    "clusters_df[f\"cluster_{step}\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0746d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 14:36:51,238 INFO Clusters DataFrame shape: (100, 5)\n",
      "2025-08-09 14:36:51,239 INFO Clusters DataFrame columns: ['object_id', 'full_name', 'full_name_embedding', 'number_of_occurrences', 'cluster_1']\n"
     ]
    }
   ],
   "source": [
    "# show index and show colums of clusters_df\n",
    "logger.info(f\"Clusters DataFrame shape: {clusters_df.shape}\")\n",
    "logger.info(f\"Clusters DataFrame columns: {clusters_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected cluster is equal to Final_Cluster, except for noise clusters where it is equal to None\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step}\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")\n",
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"cluster_{step}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f735c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clusters table\n",
    "# columns_to_keep = [\n",
    "#     \"object_id\",\n",
    "#     \"full_name\",\n",
    "#     \"number_of_occurences\",\n",
    "#     f\"cluster_{step}\",\n",
    "#]\n",
    "clusters_df.to_excel(clusters_table_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114772b",
   "metadata": {},
   "source": [
    "## Assess perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b2b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload table\n",
    "clusters_df = pd.read_excel(clusters_table_output)\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step}\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eefeef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw extracted registry names: 54335\n",
      "Number of Consolidated registry names: 2631\n",
      "Number of un-consolidated/lost (noise) registry names: 35730\n",
      "Percentage of Consolidated registry names: 34.24%\n",
      "Percentage of un-consolidated/lost (noise) registry names: 65.76%\n",
      "---\n",
      "Total number of publications with one of the 'official' extracted registry names: 163102\n",
      "Total number of publications with one of the consolidated registry names: 91561\n",
      "Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): 71541\n",
      "Percentage of lost publications: 43.86%\n"
     ]
    }
   ],
   "source": [
    "# compute % of noise clusters in corrected_cluster\n",
    "noise_clusters = clusters_df[clusters_df[f\"cluster_{step}\"].isna()]\n",
    "noise_percentage = len(noise_clusters) / len(clusters_df) * 100\n",
    "print(f\"Number of raw extracted registry names: {len(clusters_df)}\")\n",
    "# number of consolidated registry names is unique values of corrected_cluster\n",
    "print(f\"Number of Consolidated registry names: {len(clusters_df[f'cluster_{step}'].dropna().unique())}\")\n",
    "print(f\"Number of un-consolidated/lost (noise) registry names: {len(noise_clusters)}\")\n",
    "print(f\"Percentage of Consolidated registry names: {(100-noise_percentage):.2f}%\")\n",
    "print(f\"Percentage of un-consolidated/lost (noise) registry names: {noise_percentage:.2f}%\")\n",
    "print('---')\n",
    "# count total number of occurrences in clusters\n",
    "total_occurrences = clusters_df['number_of_occurrences'].sum()\n",
    "# count total number of occurences of noise clusters\n",
    "total_noise_occurrences = noise_clusters['number_of_occurrences'].sum()\n",
    "print(f\"Total number of publications with one of the 'official' extracted registry names: {total_occurrences}\")\n",
    "print(f\"Total number of publications with one of the consolidated registry names: {total_occurrences - total_noise_occurrences}\")\n",
    "print(f\"Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): {total_noise_occurrences}\")\n",
    "# print %\n",
    "print(f\"Percentage of lost publications: {total_noise_occurrences / total_occurrences * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc839ae0",
   "metadata": {},
   "source": [
    "### a. Dataset 'Any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9d1e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"cluster_{step}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8af6bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for any pairs: {'precision': 0.76, 'recall': 0.53, 'f1': 0.62, 'accuracy': 0.77}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions based on cluster mapping\n",
    "eval_df_any = apply_predictions(\n",
    "    eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "# Compute metrics (assuming ground truth is in column \"final_label\")\n",
    "metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_any_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_any.items()\n",
    "}\n",
    "print(f\"Metrics for any pairs: {metrics_any_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_any[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_any_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b992cc",
   "metadata": {},
   "source": [
    "### b. Dataset 'famous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "517195f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for famous pairs: {'precision': 0.79, 'recall': 0.65, 'f1': 0.71, 'accuracy': 0.82}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions\n",
    "eval_df_famous = apply_predictions(\n",
    "    eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "metrics_famous = compute_metrics(\n",
    "    eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    ")\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_famous_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_famous.items()\n",
    "}\n",
    "print(f\"Metrics for famous pairs: {metrics_famous_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_famous[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_famous_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d470413",
   "metadata": {},
   "source": [
    "# 2. Recompute a second step of clustering on noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2826e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the clusters_df\n",
    "step=2\n",
    "clusters_df = pd.read_excel(clusters_table_output)\n",
    "#cols to keep: [\"object_id\", \"full_name\", \"number_of_occurrences\", f\"cluster_{step-1}\"]]\n",
    "clusters_df = clusters_df[\n",
    "    [\"object_id\", \"full_name\", \"number_of_occurrences\", f\"cluster_{step-1}\"]\n",
    "].copy()\n",
    "\n",
    "# set f\"cluster_{step}\" None to 0, convert column to str(int())\n",
    "clusters_df[f\"cluster_{step-1}\"] = clusters_df[f\"cluster_{step-1}\"].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b40d52f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters of size >=20: 43\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_1</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>35730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2115</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1541</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1116</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1314</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1557</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1746</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2415</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1499</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1584</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_1  count\n",
       "0         0  35730\n",
       "1      2115     41\n",
       "2      1541     36\n",
       "3      1116     32\n",
       "4      1314     32\n",
       "5      1557     30\n",
       "6      1746     28\n",
       "7      2415     27\n",
       "8      1499     27\n",
       "9      1584     26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieve the list of clusters of size >=nmax\n",
    "n_max=20\n",
    "large_clusters = clusters_df[f\"cluster_{step-1}\"].value_counts()\n",
    "large_clusters = large_clusters[large_clusters >= n_max].index.tolist()\n",
    "# show cluster_{step-1} value counts top 30, in pandas dataframe\n",
    "large_clusters_df = (\n",
    "    clusters_df[f\"cluster_{step-1}\"].value_counts().reset_index()\n",
    ")\n",
    "large_clusters_df.columns = [f\"cluster_{step-1}\", \"count\"]\n",
    "print(f\"Clusters of size >={n_max}: {len(large_clusters)}\")\n",
    "display(large_clusters_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c59864",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.vstack(embeddings_df[\"full_name_embedding\"].values)\n",
    "\n",
    "min_cluster_size=4\n",
    "min_samples=4\n",
    "cluster_selection_epsilon=0.0\n",
    "max_cluster_size=30\n",
    "metric=\"euclidean\"\n",
    "n_jobs=-1\n",
    "cluster_selection_method=\"leaf\"\n",
    "store_centers=\"medoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ed37fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing large clusters:   0%|          | 0/43 [00:00<?, ?it/s]2025-08-07 22:54:04,240 INFO HDBSCAN completed in 1237.32 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:   2%|▏         | 1/43 [20:37<14:26:10, 1237.40s/it]2025-08-07 22:54:04,290 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,318 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,345 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,374 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  12%|█▏        | 5/43 [20:37<1:56:44, 184.32s/it]  2025-08-07 22:54:04,402 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,429 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,456 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,484 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  21%|██        | 9/43 [20:37<47:08, 83.20s/it]   2025-08-07 22:54:04,513 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,541 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,568 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,597 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  30%|███       | 13/43 [20:37<23:19, 46.65s/it]2025-08-07 22:54:04,625 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,651 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,682 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,709 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  40%|███▉      | 17/43 [20:37<12:25, 28.67s/it]2025-08-07 22:54:04,737 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,764 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,791 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,817 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  49%|████▉     | 21/43 [20:37<06:46, 18.50s/it]2025-08-07 22:54:04,845 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,874 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,900 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,930 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  58%|█████▊    | 25/43 [20:38<03:41, 12.28s/it]2025-08-07 22:54:04,957 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:04,987 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,013 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,040 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  67%|██████▋   | 29/43 [20:38<01:56,  8.30s/it]2025-08-07 22:54:05,067 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,093 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,120 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,147 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  77%|███████▋  | 33/43 [20:38<00:56,  5.68s/it]2025-08-07 22:54:05,174 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,201 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,229 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,255 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  86%|████████▌ | 37/43 [20:38<00:23,  3.92s/it]2025-08-07 22:54:05,282 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,308 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,333 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,361 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters:  95%|█████████▌| 41/43 [20:38<00:05,  2.72s/it]2025-08-07 22:54:05,387 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "2025-08-07 22:54:05,413 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=4, min_samples=4.\n",
      "Processing large clusters: 100%|██████████| 43/43 [20:38<00:00, 28.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# for each large cluster or noise, rerun an hdbscan clustering on the given cluster\n",
    "# create a column 'subcluster' in clusters_df\n",
    "# then create a new column 'cluster_{step}' that is str(cluster_{step-1}) + '_' + str(subcluster)\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step-1}\"]\n",
    "for cluster in tqdm(large_clusters, desc=\"Processing large clusters\"):\n",
    "    # get the indices of the current cluster\n",
    "    indices = clusters_df[clusters_df[f\"cluster_{step-1}\"] == cluster].index.tolist()\n",
    "    # get the embeddings for the current cluster\n",
    "    cluster_embeddings = embeddings[indices]\n",
    "    \n",
    "    # run hdbscan on the current cluster\n",
    "    sub_labels, _ = run_hdbscan(\n",
    "        cluster_embeddings,\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        max_cluster_size=max_cluster_size,\n",
    "        metric=metric,\n",
    "        n_jobs=n_jobs,\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "        store_centers=store_centers,\n",
    "    )\n",
    "    \n",
    "    # update the clusters_df with the new subcluster labels\n",
    "    clusters_df.loc[indices, f\"subcluster_{step}\"] = sub_labels\n",
    "    # compose a list of new cluster ids being str(cluster) + '_' + str(subcluster)\n",
    "    new_labels = str(cluster) + '_' + sub_labels.astype(str)\n",
    "    clusters_df.loc[indices, f\"cluster_{step}\"] = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d35a9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step=2\n",
    "# new_labels = str(cluster) + '_' + sub_labels.astype(str)\n",
    "# # show top 5 labels\n",
    "# print (f\"New labels for step {step}: {new_labels[:5]}\")\n",
    "# clusters_df.loc[indices, f\"cluster_{step}\"] = new_labels\n",
    "# # display(clusters_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d9baf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters in step 2: 3260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>31249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_270</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_503</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2115_0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_500</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0_429</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0_568</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1541_0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0_368</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1314_0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_2  count\n",
       "0       0_0  31249\n",
       "1     0_270     50\n",
       "2     0_503     44\n",
       "3    2115_0     41\n",
       "4     0_500     41\n",
       "5     0_429     40\n",
       "6     0_568     36\n",
       "7    1541_0     36\n",
       "8     0_368     34\n",
       "9    1314_0     32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display top 10 occuring clusters\n",
    "clusters_step_2 = clusters_df[f\"cluster_{step}\"].value_counts().reset_index()\n",
    "clusters_step_2.columns = [f\"cluster_{step}\", \"count\"]\n",
    "print(f\"Number of clusters in step {step}: {len(clusters_step_2)}\")\n",
    "display(clusters_step_2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8bcd58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected cluster is equal to Final_Cluster, except for noise clusters where it is equal to None\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step}\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")\n",
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"cluster_{step}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13b230fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clusters table\n",
    "# columns_to_keep = [\n",
    "#     \"object_id\",\n",
    "#     \"full_name\",\n",
    "#     \"number_of_occurences\",\n",
    "#     f\"cluster_{step}\",\n",
    "#]\n",
    "clusters_df.to_excel(clusters_table_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9699c0",
   "metadata": {},
   "source": [
    "## Assess perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d905b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw extracted registry names: 54335\n",
      "Number of Consolidated registry names: 3217\n",
      "Number of un-consolidated/lost (noise) registry names: 32245\n",
      "Percentage of Consolidated registry names: 40.66%\n",
      "Percentage of un-consolidated/lost (noise) registry names: 59.34%\n",
      "---\n",
      "Total number of publications with one of the 'official' extracted registry names: 163102\n",
      "Total number of publications with one of the consolidated registry names: 95197\n",
      "Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): 67905\n",
      "Percentage of lost publications: 41.63%\n"
     ]
    }
   ],
   "source": [
    "# compute % of noise clusters in corrected_cluster\n",
    "noise_clusters = clusters_df[clusters_df[f\"cluster_{step}\"].isna()]\n",
    "noise_percentage = len(noise_clusters) / len(clusters_df) * 100\n",
    "print(f\"Number of raw extracted registry names: {len(clusters_df)}\")\n",
    "# number of consolidated registry names is unique values of corrected_cluster\n",
    "print(f\"Number of Consolidated registry names: {len(clusters_df[f'cluster_{step}'].dropna().unique())}\")\n",
    "print(f\"Number of un-consolidated/lost (noise) registry names: {len(noise_clusters)}\")\n",
    "print(f\"Percentage of Consolidated registry names: {(100-noise_percentage):.2f}%\")\n",
    "print(f\"Percentage of un-consolidated/lost (noise) registry names: {noise_percentage:.2f}%\")\n",
    "print('---')\n",
    "# count total number of occurrences in clusters\n",
    "total_occurrences = clusters_df['number_of_occurrences'].sum()\n",
    "# count total number of occurences of noise clusters\n",
    "total_noise_occurrences = noise_clusters['number_of_occurrences'].sum()\n",
    "print(f\"Total number of publications with one of the 'official' extracted registry names: {total_occurrences}\")\n",
    "print(f\"Total number of publications with one of the consolidated registry names: {total_occurrences - total_noise_occurrences}\")\n",
    "print(f\"Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): {total_noise_occurrences}\")\n",
    "# print %\n",
    "print(f\"Percentage of lost publications: {total_noise_occurrences / total_occurrences * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7558890",
   "metadata": {},
   "source": [
    "### a. Dataset 'Any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33db7641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for any pairs: {'precision': 0.73, 'recall': 0.55, 'f1': 0.62, 'accuracy': 0.76}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions based on cluster mapping\n",
    "eval_df_any = apply_predictions(\n",
    "    eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "# Compute metrics (assuming ground truth is in column \"final_label\")\n",
    "metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_any_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_any.items()\n",
    "}\n",
    "print(f\"Metrics for any pairs: {metrics_any_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_any[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_any_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0822ebf",
   "metadata": {},
   "source": [
    "### b. Dataset 'famous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "245696e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for famous pairs: {'precision': 0.76, 'recall': 0.6, 'f1': 0.67, 'accuracy': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions\n",
    "eval_df_famous = apply_predictions(\n",
    "    eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "metrics_famous = compute_metrics(\n",
    "    eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    ")\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_famous_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_famous.items()\n",
    "}\n",
    "print(f\"Metrics for famous pairs: {metrics_famous_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_famous[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_famous_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a014ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb8bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P05_refine_dedup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
