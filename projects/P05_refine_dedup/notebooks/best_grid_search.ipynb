{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa75539",
   "metadata": {},
   "source": [
    "# Grid Search for Best Epsilon Configuration\n",
    "This notebook performs a grid search over multiple epsilon values for HDBSCAN clustering, evaluating each configuration on two datasets and saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2119f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.pyenv/versions/3.11.11/envs/P05_refine_dedup_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Set working directory (adjust if needed)\n",
    "working_dir = '/home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup'\n",
    "os.chdir(working_dir)\n",
    "print(f'Changed working directory to {working_dir}')\n",
    "from src.p05_refine_dedup import config\n",
    "from src.p05_refine_dedup.utils.utils import (\n",
    "    is_noise,\n",
    "    run_hdbscan,\n",
    "    apply_predictions,\n",
    "    compute_metrics,\n",
    ")\n",
    "from src.p05_refine_dedup.utils.s3_io_functions import (\n",
    "    load_parquet_from_s3,\n",
    ")\n",
    "\n",
    "output_dir = Path(\"data/W03/from_notebooks/R06_additional_grid_search/v2\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_xlsx = output_dir / 'grid_search_results.xlsx'\n",
    "best_config_json = output_dir / 'best_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cabcf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data will be loaded from s3://s3-common-dev20231214174437248800000002/registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "s3_input_embeddings = 'registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet'\n",
    "bucket_name = config.BUCKET_NAME_DEV\n",
    "folder_path = s3_input_embeddings.rsplit('/', 1)[0]\n",
    "file_name = s3_input_embeddings.rsplit('/', 1)[-1]\n",
    "embeddings_df = load_parquet_from_s3(\n",
    "    bucket_name=bucket_name,\n",
    "    folder_path=folder_path,\n",
    "    file_name=file_name,\n",
    ")\n",
    "clusters_table_xlsx = 'data/W02/R02_evaluate_model_performance/clusters_table.xlsx'\n",
    "clusters_df = pd.read_excel(clusters_table_xlsx)\n",
    "clusters_df = clusters_df.merge(\n",
    "    embeddings_df[['full_name', 'full_name_embedding']],\n",
    "    on='full_name',\n",
    "    how='left'\n",
    ")\n",
    "clusters_df.rename(columns={'Final_Cluster': 'cluster_0'}, inplace=True)\n",
    "\n",
    "evaluation_dataset_any = 'data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/assessed_pairs_v1.xlsx'\n",
    "evaluation_dataset_famous = 'data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/famous_close_assessed_pairs_v1.xlsx'\n",
    "eval_df_any = pd.read_excel(evaluation_dataset_any)\n",
    "eval_df_famous = pd.read_excel(evaluation_dataset_famous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27b9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epsilon ranges for each cluster type\n",
    "eps_grid = {\n",
    "    '0_0': np.arange(0.44, 0.51, 0.01),\n",
    "    'start_0_': np.arange(0.34, 0.44, 0.01),\n",
    "    'end__0': np.arange(0.28, 0.33, 0.01),\n",
    "    'other': np.arange(0.19, 0.27, 0.01),\n",
    "}\n",
    "# convert array to list of str()\n",
    "for key in eps_grid:\n",
    "    eps_grid[key] = [str(round(x, 2)) for x in eps_grid[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60568134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to get epsilon for a cluster given the current config\n",
    "def get_epsilon(cluster_id, eps_config):\n",
    "    if cluster_id == '0_0':\n",
    "        return eps_config['0_0']\n",
    "    elif cluster_id.startswith('0_'):\n",
    "        return eps_config['start_0_']\n",
    "    elif cluster_id.endswith('_0'):\n",
    "        return eps_config['end__0']\n",
    "    else:\n",
    "        return eps_config['other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2773674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one dictionary of new cluseters ids for each cluster type\n",
    "# dict_0_0 = {\n",
    "#     0.44:[\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '0',\n",
    "#          'cluster_1': '0_0_0'\n",
    "#          },\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '1',\n",
    "#          'cluster_1': '0_0_1'\n",
    "#          },\n",
    "#          {'full_name': 'name_2',\n",
    "#           'cluster_0': '0_0',\n",
    "#           'subcluster': '2',\n",
    "#           'cluster_1': '0_0_2'\n",
    "#           }\n",
    "#           ...\n",
    "#          ],\n",
    "#     0.45:[\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '0',\n",
    "#          'cluster_1': '0_0_0'\n",
    "#          },\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '1',\n",
    "#          'cluster_1': '0_0_1'\n",
    "#          },\n",
    "#          {'full_name': 'name_2',\n",
    "#           'cluster_0': '0_0',\n",
    "#           'subcluster': '2',\n",
    "#           'cluster_1': '0_0_2'\n",
    "#           }\n",
    "#           ...\n",
    "#          ],\n",
    "#     ...\n",
    "# }\n",
    "def filter_on_cluster_type(clusters_df, cluster_type):\n",
    "    if cluster_type == '0_0':\n",
    "        return clusters_df[clusters_df['cluster_0'] == cluster_type]\n",
    "    elif cluster_type == 'start_0_':\n",
    "        # starting with '0_' and not ending with '_0'\n",
    "        return clusters_df[(clusters_df['cluster_0'].str.startswith('0_')) &\n",
    "                           (~clusters_df['cluster_0'].str.endswith('_0'))]\n",
    "    elif cluster_type == 'end__0':\n",
    "        # ending with '_0' and not starting with '0_'\n",
    "        return clusters_df[clusters_df['cluster_0'].str.endswith('_0') &\n",
    "                           (~clusters_df['cluster_0'].str.startswith('0_'))]\n",
    "    elif cluster_type == 'other':\n",
    "        return clusters_df[~clusters_df['cluster_0'].str.startswith('0_') &\n",
    "                           ~clusters_df['cluster_0'].str.endswith('_0')]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown cluster type: {cluster_type}\")\n",
    "\n",
    "# Initiate dict_0_0 with all espilons and full_name and cluster_0 only\n",
    "def create_initial_dict(clusters_df, cluster_type):\n",
    "    initial_dict = {}\n",
    "    for eps in eps_grid[cluster_type]:\n",
    "        clusters_df_filtered = filter_on_cluster_type(clusters_df, cluster_type)\n",
    "        initial_dict[eps] = clusters_df_filtered[['full_name', 'cluster_0']].copy()\n",
    "        # set subcluster and cluster_1 to None\n",
    "        initial_dict[eps]['subcluster'] = None\n",
    "        initial_dict[eps]['cluster_1'] = None\n",
    "    return initial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5de6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 20\n",
    "large_clusters = clusters_df['cluster_0'].value_counts()[clusters_df['cluster_0'].value_counts() >= n_max].index.tolist()\n",
    "# filter on large clusters\n",
    "large_clusters_df = clusters_df[clusters_df['cluster_0'].isin(large_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27465cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate dictionaries for each cluster type\n",
    "dict_0_0 = create_initial_dict(large_clusters_df, '0_0')\n",
    "dict_start_0_ = create_initial_dict(large_clusters_df, 'start_0_')\n",
    "dict_end__0 = create_initial_dict(large_clusters_df, 'end__0')\n",
    "dict_other = create_initial_dict(large_clusters_df, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199ba06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show the first 5 rows of each dictionary\n",
    "# def show_first_five_rows(d):\n",
    "#     for eps, df in d.items():\n",
    "#         print(f\"Epsilon: {eps}\")\n",
    "#         print(df.head(), \"\\n\")\n",
    "\n",
    "# print(\"Initial dictionaries with full_name and cluster_0:\")\n",
    "# print(\"0_0\")\n",
    "# show_first_five_rows(dict_0_0)\n",
    "# print(\"start_0_\")\n",
    "# show_first_five_rows(dict_start_0_)\n",
    "# print(\"end__0\")\n",
    "# show_first_five_rows(dict_end__0)\n",
    "# print(\"other\")\n",
    "# show_first_five_rows(dict_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d3ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=2\n",
    "min_samples=2\n",
    "# cluster_selection_epsilon=0.0\n",
    "max_cluster_size=30\n",
    "metric=\"euclidean\"\n",
    "n_jobs=-1\n",
    "cluster_selection_method=\"leaf\"\n",
    "store_centers=\"medoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af310ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_clusters(clusters_df, cluster_type, results_dict):\n",
    "    # first filter on cluster type\n",
    "    clusters_df_filtered = filter_on_cluster_type(clusters_df, cluster_type)\n",
    "    # # test on 100 data points\n",
    "    # clusters_df_filtered = clusters_df_filtered.head(1000) # For testing, remove this line for full dataset\n",
    "    # retrive the list of clusters in cluster_0\n",
    "    clusters = clusters_df_filtered['cluster_0'].unique().tolist()\n",
    "\n",
    "    for eps in tqdm(eps_grid[cluster_type], desc=f\"Processing {cluster_type} clusters\"):\n",
    "        for cluster in clusters:\n",
    "            # first filter on this cluster\n",
    "            df = clusters_df_filtered[clusters_df_filtered['cluster_0'] == cluster].copy()\n",
    "            embeddings = np.vstack(df['full_name_embedding'].values)\n",
    "\n",
    "            # Add subcluster and cluster_1 columns\n",
    "            df['subcluster'] = None\n",
    "            df['cluster_1'] = None\n",
    "            \n",
    "            # Apply HDBSCAN clustering\n",
    "            labels, comp_time = run_hdbscan(\n",
    "                embeddings,\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                cluster_selection_epsilon=float(eps),\n",
    "                max_cluster_size=max_cluster_size,\n",
    "                metric=metric,\n",
    "                n_jobs=n_jobs,\n",
    "                cluster_selection_method=cluster_selection_method,\n",
    "                store_centers=store_centers,\n",
    "            )\n",
    "            \n",
    "            df['subcluster'] = labels.astype(str)  # Convert labels to string for subcluster\n",
    "            df['cluster_1'] = df['cluster_0'] + '_' + df['subcluster']\n",
    "\n",
    "            # Vectorized update: set full_name as index for both DataFrames, then update\n",
    "            updates = df[['full_name', 'subcluster', 'cluster_1']].set_index('full_name')\n",
    "            res_df = results_dict[eps].set_index('full_name')\n",
    "            res_df.update(updates)\n",
    "            results_dict[eps] = res_df.reset_index()\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "569c6f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0_0 clusters: 100%|██████████| 8/8 [18:44<00:00, 140.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# test on 100 data points on '0_0' clusters\n",
    "dict_0_0 = process_clusters(large_clusters_df, '0_0', dict_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1afb1bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing start_0_ clusters:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing start_0_ clusters: 100%|██████████| 10/10 [04:12<00:00, 25.24s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_start_0_ = process_clusters(large_clusters_df, 'start_0_', dict_start_0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6286c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing end__0 clusters: 100%|██████████| 5/5 [00:06<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_end__0 = process_clusters(large_clusters_df, 'end__0', dict_end__0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07d9c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing other clusters: 100%|██████████| 9/9 [00:23<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_other = process_clusters(large_clusters_df, 'other', dict_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "682d6019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dict_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4238166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each dictionary to a json file dict_0_0, dict_start_0_, dict_end__0, dict_other\n",
    "# not looping on eps values, save direclty all eps values in on single json file\n",
    "def save_dict_to_json(data_dict, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data_dict, f, indent=4)\n",
    "# transform each object (dataframe) in the dict to a dict\n",
    "dict_0_0 = {eps: df.to_dict(orient='records') for eps, df in dict_0_0.items()}\n",
    "dict_start_0_ = {eps: df.to_dict(orient='records') for eps, df in dict_start_0_.items()}\n",
    "dict_end__0 = {eps: df.to_dict(orient='records') for eps, df in dict_end__0.items()}\n",
    "dict_other = {eps: df.to_dict(orient='records') for eps, df in dict_other.items()}\n",
    "# Save dictionaries to json files\n",
    "save_dict_to_json(dict_0_0, output_dir / 'dict_0_0.json')\n",
    "save_dict_to_json(dict_start_0_, output_dir / 'dict_start_0_.json')\n",
    "save_dict_to_json(dict_end__0, output_dir / 'dict_end__0.json')\n",
    "save_dict_to_json(dict_other, output_dir / 'dict_other.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd02775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for any pairs: {'precision': 0.72, 'recall': 0.79, 'f1': 0.76, 'accuracy': 0.81}\n",
      "Metrics for famous pairs: {'precision': 0.81, 'recall': 0.77, 'f1': 0.79, 'accuracy': 0.86}\n"
     ]
    }
   ],
   "source": [
    "clusters_df[f\"corrected_cluster\"] = clusters_df[f\"corrected_cluster\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")\n",
    "# recompute current performance metrics\n",
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"corrected_cluster\"]))\n",
    "# Apply predictions based on cluster mapping\n",
    "eval_df_any = apply_predictions(\n",
    "    eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "# Compute metrics (assuming ground truth is in column \"final_label\")\n",
    "metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_any_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_any.items()\n",
    "}\n",
    "print(f\"Metrics for any pairs: {metrics_any_to_print}\")\n",
    "\n",
    "# Apply predictions\n",
    "eval_df_famous = apply_predictions(\n",
    "    eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "metrics_famous = compute_metrics(\n",
    "    eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    ")\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_famous_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_famous.items()\n",
    "}\n",
    "print(f\"Metrics for famous pairs: {metrics_famous_to_print}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc62fc5",
   "metadata": {},
   "source": [
    "# reload an compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37203c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the dictionaries from json files\n",
    "def load_dict_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "dict_0_0 = load_dict_from_json(output_dir / 'dict_0_0.json')\n",
    "dict_start_0_ = load_dict_from_json(output_dir / 'dict_start_0_.json')\n",
    "dict_end__0 = load_dict_from_json(output_dir / 'dict_end__0.json')\n",
    "dict_other = load_dict_from_json(output_dir / 'dict_other.json')\n",
    "\n",
    "# transform each object (dict) in the dict to a dataframe\n",
    "def transform_dict_to_df(data_dict):\n",
    "    return {eps: pd.DataFrame(records) for eps, records in data_dict.items()}\n",
    "dict_0_0 = transform_dict_to_df(dict_0_0)\n",
    "dict_start_0_ = transform_dict_to_df(dict_start_0_)\n",
    "dict_end__0 = transform_dict_to_df(dict_end__0)\n",
    "dict_other = transform_dict_to_df(dict_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8a56624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 0/3600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1911/1047643353.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  grid_search_results = pd.concat(\n",
      "Processing epsilon combinations:   0%|          | 1/3600 [00:00<35:18,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best precision_mean: 0.7 with eps: ('0.44', '0.34', '0.28', '0.19')\n",
      "--- New best f1_mean: 0.27 with eps: ('0.44', '0.34', '0.28', '0.19')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 2/3600 [00:01<35:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best precision_mean: 0.7 with eps: ('0.44', '0.34', '0.28', '0.2')\n",
      "--- New best f1_mean: 0.27 with eps: ('0.44', '0.34', '0.28', '0.2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 3/3600 [00:01<34:57,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.29 with eps: ('0.44', '0.34', '0.28', '0.21')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 4/3600 [00:02<34:54,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.3 with eps: ('0.44', '0.34', '0.28', '0.22')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 5/3600 [00:02<34:59,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.3 with eps: ('0.44', '0.34', '0.28', '0.23')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 6/3600 [00:03<34:53,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.31 with eps: ('0.44', '0.34', '0.28', '0.24')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 7/3600 [00:04<35:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.32 with eps: ('0.44', '0.34', '0.28', '0.25')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 8/3600 [00:04<34:57,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.33 with eps: ('0.44', '0.34', '0.28', '0.26')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 9/3600 [00:05<34:54,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.33 with eps: ('0.44', '0.34', '0.28', '0.27')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   1%|          | 27/3600 [00:15<34:33,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.33 with eps: ('0.44', '0.34', '0.3', '0.27')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   1%|▏         | 45/3600 [00:26<34:21,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.33 with eps: ('0.44', '0.34', '0.32', '0.27')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations: 100%|██████████| 3600/3600 [34:32<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration: {'eps_0_0': 0.44, 'eps_start_0_': 0.34, 'eps_end__0': 0.32, 'eps_other': 0.27, 'f1_mean': 0.33, 'precision_mean': 0.68, 'recall_mean': 0.22, 'f1_any': 0.23, 'precision_any': 0.54, 'recall_any': 0.15, 'f1_famous': 0.42, 'precision_famous': 0.83, 'recall_famous': 0.28}\n"
     ]
    }
   ],
   "source": [
    "# initiate grid_search_results with columns eps_0_0, eps_start_0_, eps_end__0, eps_other, f1_any, precision_any, recall_any, f1_famous, precision_famous, recall_famous, f1_mean, precision_mean, recall_mean,\n",
    "grid_search_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"eps_0_0\",\n",
    "        \"eps_start_0_\",\n",
    "        \"eps_end__0\",\n",
    "        \"eps_other\",\n",
    "        \"f1_any\",\n",
    "        \"precision_any\",\n",
    "        \"recall_any\",\n",
    "        \"f1_famous\",\n",
    "        \"precision_famous\",\n",
    "        \"recall_famous\",\n",
    "        \"f1_mean\",\n",
    "        \"precision_mean\",\n",
    "        \"recall_mean\",\n",
    "    ]\n",
    ")\n",
    "# finally loop on all combiinations of epsilons in all dictionaries\n",
    "from itertools import product\n",
    "\n",
    "# Create a list of all combinations of epsilons\n",
    "eps_combinations = list(\n",
    "    product(\n",
    "        eps_grid[\"0_0\"], eps_grid[\"start_0_\"], eps_grid[\"end__0\"], eps_grid[\"other\"]\n",
    "    )\n",
    ")\n",
    "best_f1_mean = -1\n",
    "best_precision_mean= -1\n",
    "best_config = None\n",
    "# Loop through each combination of epsilons\n",
    "for eps_combination in tqdm(eps_combinations, desc=\"Processing epsilon combinations\"):\n",
    "    clusters_copy = clusters_df.copy()\n",
    "    clusters_copy[\"subcluster\"] = None\n",
    "    clusters_copy[\"cluster_1\"] = None\n",
    "    clusters_copy = clusters_copy.set_index(\"full_name\")\n",
    "\n",
    "    eps_0_0, eps_start_0_, eps_end__0, eps_other = eps_combination\n",
    "    # Create a new row for the results DataFrame\n",
    "    new_row = {\n",
    "        \"eps_0_0\": eps_0_0,\n",
    "        \"eps_start_0_\": eps_start_0_,\n",
    "        \"eps_end__0\": eps_end__0,\n",
    "        \"eps_other\": eps_other,\n",
    "    }\n",
    "\n",
    "    # Update clusters_df with the new cluster_1 and subcluster for all dictionaries matching on 'full_name'\n",
    "    for cluster_type, dict_data, eps in zip(\n",
    "        [\"0_0\", \"start_0_\", \"end__0\", \"other\"],\n",
    "        [dict_0_0, dict_start_0_, dict_end__0, dict_other],\n",
    "        [eps_0_0, eps_start_0_, eps_end__0, eps_other],\n",
    "    ):\n",
    "        df = dict_data[eps].copy().set_index(\"full_name\")\n",
    "        clusters_copy.update(df[[\"subcluster\", \"cluster_1\"]])\n",
    "\n",
    "    # reset normal index\n",
    "    clusters_copy.reset_index(inplace=True)\n",
    "    \n",
    "    clusters_copy[\"cluster_1\"] = clusters_copy[\"cluster_1\"].apply(\n",
    "        lambda x: None if is_noise(x) else x\n",
    "    )\n",
    "    cluster_map = dict(zip(clusters_copy[\"full_name\"], clusters_copy[\"cluster_1\"]))\n",
    "\n",
    "    # Compute metrics on the updated clusters_df\n",
    "    # Apply predictions\n",
    "    eval_df_any = apply_predictions(\n",
    "        eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    "    )\n",
    "    metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "    eval_df_famous = apply_predictions(\n",
    "        eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    "    )\n",
    "    metrics_famous = compute_metrics(\n",
    "        eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    "    )\n",
    "\n",
    "    # Add metrics to the new row\n",
    "    new_row.update(\n",
    "        {\n",
    "            \"f1_any\": metrics_any[\"f1\"],\n",
    "            \"precision_any\": metrics_any[\"precision\"],\n",
    "            \"recall_any\": metrics_any[\"recall\"],\n",
    "            \"f1_famous\": metrics_famous[\"f1\"],\n",
    "            \"precision_famous\": metrics_famous[\"precision\"],\n",
    "            \"recall_famous\": metrics_famous[\"recall\"],\n",
    "            \"f1_mean\": (metrics_any[\"f1\"] + metrics_famous[\"f1\"]) / 2,\n",
    "            \"precision_mean\": (metrics_any[\"precision\"] + metrics_famous[\"precision\"])\n",
    "            / 2,\n",
    "            \"recall_mean\": (metrics_any[\"recall\"] + metrics_famous[\"recall\"]) / 2,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check if this is the best configuration so far. if yes, then update best_config and best_f1_mean\n",
    "    if new_row[\"precision_mean\"] > best_precision_mean:\n",
    "        print(f\"--- New best precision_mean: {round(new_row['precision_mean'],2)} with eps: {eps_combination}\")\n",
    "        best_precision_mean = new_row[\"precision_mean\"]\n",
    "        \n",
    "    if new_row[\"f1_mean\"] > best_f1_mean:\n",
    "        print(f\"--- New best f1_mean: {round(new_row['f1_mean'],2)} with eps: {eps_combination}\")\n",
    "        best_f1_mean = new_row[\"f1_mean\"]\n",
    "        best_config = {\n",
    "            \"eps_0_0\": eps_0_0,\n",
    "            \"eps_start_0_\": eps_start_0_,\n",
    "            \"eps_end__0\": eps_end__0,\n",
    "            \"eps_other\": eps_other,\n",
    "            \"f1_mean\": best_f1_mean,\n",
    "            \"precision_mean\": new_row[\"precision_mean\"],\n",
    "            \"recall_mean\": new_row[\"recall_mean\"],\n",
    "            \"f1_any\": new_row[\"f1_any\"],\n",
    "            \"precision_any\": new_row[\"precision_any\"],\n",
    "            \"recall_any\": new_row[\"recall_any\"],\n",
    "            \"f1_famous\": new_row[\"f1_famous\"],\n",
    "            \"precision_famous\": new_row[\"precision_famous\"],\n",
    "            \"recall_famous\": new_row[\"recall_famous\"],\n",
    "        }\n",
    "        # convert all values in best_config to float with 2 decimal places\n",
    "        best_config = {k: round(float(v), 2) for k, v in best_config.items()}\n",
    "\n",
    "    # Append the new row to the results DataFrame\n",
    "    grid_search_results = pd.concat(\n",
    "        [grid_search_results, pd.DataFrame([new_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Save grid search results to Excel\n",
    "grid_search_results.to_excel(results_xlsx, index=False)\n",
    "# print and save best configuration to JSON\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "with open(best_config_json, 'w') as f:\n",
    "    json.dump(best_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc368690",
   "metadata": {},
   "source": [
    "## Grid search complete\n",
    "- All results are saved in `grid_search_results.xlsx`.\n",
    "- The best configuration is saved in `best_config.json`.\n",
    "- You can now use the best configuration for further clustering and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f06eba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P05_refine_dedup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
