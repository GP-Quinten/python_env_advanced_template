{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d5d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import llm_backends\n",
    "from llm_backends.cache import DiskCacheStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f5af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"temperature\": 1.0,\n",
    "    \"max_tokens\": 52,\n",
    "    \"random_seed\": 42,\n",
    "    \"response_format\": {\n",
    "        \"type\": \"json_object\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aba8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 10:01:05,234 - llm_backends.cache.disk.DiskCacheStorage - INFO - Disk cache initialized at: /Users/bkopin/dev/more_europa/registry_catalog_data_science/src/llm_backends/llm_backends/.cache\n",
      "2025-06-26 10:01:05,259 - llm_backends.cache.disk.DiskCacheStorage - INFO - Attempting to retrieve cache for key: 9dd072a192ff611fc04f43c5876d626fc0d3f12fb655c143e1149aef52e70230\n",
      "2025-06-26 10:01:05,260 - llm_backends.cache.disk.DiskCacheStorage - INFO - Cache file not found for key: 9dd072a192ff611fc04f43c5876d626fc0d3f12fb655c143e1149aef52e70230\n",
      "2025-06-26 10:01:05,548 - llm_backends.cache.disk.DiskCacheStorage - INFO - Storing cache for key: 9dd072a192ff611fc04f43c5876d626fc0d3f12fb655c143e1149aef52e70230\n",
      "2025-06-26 10:01:05,549 - llm_backends.cache.disk.DiskCacheStorage - INFO - Cache stored successfully for key: 9dd072a192ff611fc04f43c5876d626fc0d3f12fb655c143e1149aef52e70230\n",
      "{'capital': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "from llm_backends.mistral import dummy_config\n",
    "backend = llm_backends.MistralAsyncBackend(\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    cache_storage=DiskCacheStorage()\n",
    ")\n",
    "\n",
    "raw_response = await backend.infer_one(\n",
    "    prompt_item={\"custom_id\": \"example\", \"prompt\": \"What is the capital of France? respond in JSON format.\"},\n",
    "    model_config=config,\n",
    ")\n",
    "\n",
    "response = backend._parse_response(raw_response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0904a3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 10:03:42,095 - llm_backends.cache.disk.DiskCacheStorage - INFO - Disk cache initialized at: /Users/bkopin/dev/more_europa/registry_catalog_data_science/src/llm_backends/llm_backends/.cache\n",
      "2025-06-26 10:03:42,125 - llm_backends.cache.disk.DiskCacheStorage - INFO - Attempting to retrieve cache for key: 413909001afc5048dd3331fa395b1fe97b53cfa833ed3a6bfac2c089557809ab\n",
      "2025-06-26 10:03:42,125 - llm_backends.cache.disk.DiskCacheStorage - INFO - Cache hit for key: 413909001afc5048dd3331fa395b1fe97b53cfa833ed3a6bfac2c089557809ab\n",
      "Cache hit for key: What is the capital of France? respond in JSON format.What is the largest planet in our solar system? respond in JSON format.{\"max_tokens\": 300, \"model\": \"mistral-small-latest\", \"random_seed\": 42, \"response_format\": {\"type\": \"json_object\"}, \"temperature\": 1.0}\n",
      "{'capital': 'Paris'}\n",
      "{'largest_planet': 'Jupiter', 'details': {'radius': '69,911 kilometers (43,441 miles)', 'mass': \"1.898 Ã— 10^27 kilograms (318 times Earth's mass)\", 'moons': '95 confirmed moons (as of 2023)', 'location': '5th planet from the Sun', 'type': 'Gas giant'}}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"temperature\": 1.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"random_seed\": 42,\n",
    "    \"response_format\": {\n",
    "        \"type\": \"json_object\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from llm_backends.mistral import dummy_config\n",
    "backend = llm_backends.MistralBatchBackend(\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    cache_storage=DiskCacheStorage()\n",
    ")\n",
    "\n",
    "\n",
    "items = [\n",
    "    {\"custom_id\": \"example1\", \"prompt\": \"What is the capital of France? respond in JSON format.\"},\n",
    "    {\"custom_id\": \"example2\", \"prompt\": \"What is the largest planet in our solar system? respond in JSON format.\"},\n",
    "]\n",
    "\n",
    "raw_responses = backend.infer_many(\n",
    "    prompt_items=items,\n",
    "    model_config=config,\n",
    ")\n",
    "\n",
    "for raw_response in raw_responses:\n",
    "    response = backend._parse_response(raw_response)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad153381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_backends.openai import dummy_config\n",
    "\n",
    "backend = llm_backends.OpenAIAsyncBackend(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    cache_storage=DiskCacheStorage()\n",
    ")\n",
    "\n",
    "raw_response = await backend.infer_one(\n",
    "    prompt_item={\"custom_id\": \"example\", \"prompt\": \"What is the capital of France? respond in JSON format.\"},\n",
    "    model_config=dummy_config\n",
    ")\n",
    "response = backend._parse_response(raw_response)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P00_template-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
