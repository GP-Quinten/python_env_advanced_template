{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304f9693",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4a68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.pyenv/versions/3.11.11/envs/P05_refine_dedup_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set working directory (adjust if needed)\n",
    "working_dir = '/home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup'\n",
    "os.chdir(working_dir)\n",
    "print(f'Changed working directory to {working_dir}')\n",
    "\n",
    "from src.p05_refine_dedup import config\n",
    "from src.p05_refine_dedup.utils.utils import (\n",
    "    is_noise,\n",
    "    run_hdbscan,\n",
    "    apply_predictions,\n",
    "    compute_metrics,\n",
    ")\n",
    "from src.p05_refine_dedup.utils.s3_io_functions import (\n",
    "    load_parquet_from_s3,\n",
    ")  # for loading embeddings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61214cda",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e9f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 22:38:18,845 INFO Loading embeddings from registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\n",
      "2025-08-07 22:38:18,846 WARNING Data will be loaded from s3://s3-common-dev20231214174437248800000002/registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "s3_input_embeddings = \"registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\"\n",
    "# 2. Load embeddings from S3 (parquet file)\n",
    "logger.info(f\"Loading embeddings from {s3_input_embeddings}\")\n",
    "bucket_name = config.BUCKET_NAME_DEV\n",
    "# folder_path = s3_input_embeddings, file_name = last part\n",
    "folder_path = s3_input_embeddings.rsplit(\"/\", 1)[0]\n",
    "file_name = s3_input_embeddings.rsplit(\"/\", 1)[-1]\n",
    "embeddings_df = load_parquet_from_s3(\n",
    "    bucket_name=bucket_name,\n",
    "    folder_path=folder_path,\n",
    "    file_name=file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5fc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset_any=\"data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/assessed_pairs_v1.xlsx\"\n",
    "evaluation_dataset_famous=\"data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/famous_close_assessed_pairs_v1.xlsx\"\n",
    "\n",
    "# best_config=\"etc/best_config.json\", # this is the best config obtained from the previous step, in json format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170c1887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load evaluation dataset (any pairs) from data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/assessed_pairs_v1.xlsx\n",
      "Load evaluation dataset (famous pairs) from data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/famous_close_assessed_pairs_v1.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(f\"Load evaluation dataset (any pairs) from {evaluation_dataset_any}\")\n",
    "eval_df_any = pd.read_excel(evaluation_dataset_any)\n",
    "print(f\"Load evaluation dataset (famous pairs) from {evaluation_dataset_famous}\")\n",
    "eval_df_famous = pd.read_excel(evaluation_dataset_famous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4dd21",
   "metadata": {},
   "source": [
    "# 1. Compute first global clustering with hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c601d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define outputs\n",
    "version=\"v1\"\n",
    "step=1\n",
    "clusters_table_output = f\"data/W03/from_notebooks/R03_evaluate_hdbscan_eom_model_performance/{version}/clusters_table.xlsx\"\n",
    "# make sure the output directory exists\n",
    "output_dir = Path(clusters_table_output).parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prediction_any_results_xlsx=f\"data/W03/from_notebooks/R03_evaluate_hdbscan_eom_model_performance/{version}/prediction_results_any_reg.xlsx\"\n",
    "prediction_famous_results_xlsx=f\"data/W03/from_notebooks/R03_evaluate_hdbscan_eom_model_performance/{version}/prediction_results_famous_reg.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c85994",
   "metadata": {},
   "source": [
    "## Apply HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ca9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selet top n rows for testing\n",
    "# n_rows = 100\n",
    "# embeddings_df = embeddings_df.head(n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b784fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=2\n",
    "min_samples=2\n",
    "cluster_selection_epsilon=0.0\n",
    "max_cluster_size=30\n",
    "metric=\"euclidean\"\n",
    "n_jobs=-1\n",
    "cluster_selection_method=\"eom\"\n",
    "store_centers=\"medoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "249c1629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 23:22:49,054 INFO HDBSCAN completed in 2664.16 seconds with min_cluster_size=2, min_samples=2.\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.vstack(embeddings_df[\"full_name_embedding\"].values)\n",
    "labels, comp_time = run_hdbscan(\n",
    "    embeddings,\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "    max_cluster_size=max_cluster_size,\n",
    "    metric=metric,\n",
    "    n_jobs=n_jobs,\n",
    "    cluster_selection_method=cluster_selection_method,\n",
    "    store_centers=store_centers,\n",
    ")\n",
    "clusters_df = embeddings_df.copy()\n",
    "clusters_df[f\"cluster_{step}\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4d2ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected cluster is equal to Final_Cluster, except for noise clusters where it is equal to None\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step}\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")\n",
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"cluster_{step}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f735c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clusters table\n",
    "# columns_to_keep = [\n",
    "#     \"object_id\",\n",
    "#     \"full_name\",\n",
    "#     \"number_of_occurences\",\n",
    "#     f\"cluster_{step}\",\n",
    "#]\n",
    "clusters_df.to_excel(clusters_table_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114772b",
   "metadata": {},
   "source": [
    "## Assess perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b2b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload table\n",
    "clusters_df = pd.read_excel(clusters_table_output)\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step}\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eefeef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw extracted registry names: 54335\n",
      "Number of Consolidated registry names: 12462\n",
      "Number of un-consolidated/lost (noise) registry names: 17870\n",
      "Percentage of Consolidated registry names: 67.11%\n",
      "Percentage of un-consolidated/lost (noise) registry names: 32.89%\n",
      "---\n",
      "Total number of publications with one of the 'official' extracted registry names: 163102\n",
      "Total number of publications with one of the consolidated registry names: 134139\n",
      "Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): 28963\n",
      "Percentage of lost publications: 17.76%\n"
     ]
    }
   ],
   "source": [
    "# compute % of noise clusters in corrected_cluster\n",
    "noise_clusters = clusters_df[clusters_df[f\"cluster_{step}\"].isna()]\n",
    "noise_percentage = len(noise_clusters) / len(clusters_df) * 100\n",
    "print(f\"Number of raw extracted registry names: {len(clusters_df)}\")\n",
    "# number of consolidated registry names is unique values of corrected_cluster\n",
    "print(f\"Number of Consolidated registry names: {len(clusters_df[f'cluster_{step}'].dropna().unique())}\")\n",
    "print(f\"Number of un-consolidated/lost (noise) registry names: {len(noise_clusters)}\")\n",
    "print(f\"Percentage of Consolidated registry names: {(100-noise_percentage):.2f}%\")\n",
    "print(f\"Percentage of un-consolidated/lost (noise) registry names: {noise_percentage:.2f}%\")\n",
    "print('---')\n",
    "# count total number of occurrences in clusters\n",
    "total_occurrences = clusters_df['number_of_occurrences'].sum()\n",
    "# count total number of occurences of noise clusters\n",
    "total_noise_occurrences = noise_clusters['number_of_occurrences'].sum()\n",
    "print(f\"Total number of publications with one of the 'official' extracted registry names: {total_occurrences}\")\n",
    "print(f\"Total number of publications with one of the consolidated registry names: {total_occurrences - total_noise_occurrences}\")\n",
    "print(f\"Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): {total_noise_occurrences}\")\n",
    "# print %\n",
    "print(f\"Percentage of lost publications: {total_noise_occurrences / total_occurrences * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc839ae0",
   "metadata": {},
   "source": [
    "### a. Dataset 'Any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d1e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"cluster_{step}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af6bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for any pairs: {'precision': 0.71, 'recall': 0.75, 'f1': 0.73, 'accuracy': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions based on cluster mapping\n",
    "eval_df_any = apply_predictions(\n",
    "    eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "# Compute metrics (assuming ground truth is in column \"final_label\")\n",
    "metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_any_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_any.items()\n",
    "}\n",
    "print(f\"Metrics for any pairs: {metrics_any_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_any[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_any_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b992cc",
   "metadata": {},
   "source": [
    "### b. Dataset 'famous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517195f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for famous pairs: {'precision': 0.86, 'recall': 0.49, 'f1': 0.63, 'accuracy': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions\n",
    "eval_df_famous = apply_predictions(\n",
    "    eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "metrics_famous = compute_metrics(\n",
    "    eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    ")\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_famous_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_famous.items()\n",
    "}\n",
    "print(f\"Metrics for famous pairs: {metrics_famous_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_famous[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_famous_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d470413",
   "metadata": {},
   "source": [
    "# 2. Recompute a second step of clustering on noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2826e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the clusters_df\n",
    "step=2\n",
    "clusters_df = pd.read_excel(clusters_table_output)\n",
    "#cols to keep: [\"object_id\", \"full_name\", \"number_of_occurrences\", f\"cluster_{step-1}\"]]\n",
    "clusters_df = clusters_df[\n",
    "    [\"object_id\", \"full_name\", \"number_of_occurrences\", f\"cluster_{step-1}\"]\n",
    "].copy()\n",
    "\n",
    "# set f\"cluster_{step}\" None to 0, convert column to str(int())\n",
    "clusters_df[f\"cluster_{step-1}\"] = clusters_df[f\"cluster_{step-1}\"].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b40d52f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters of size >=20: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_1</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10357</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9379</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12030</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8818</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9547</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10385</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8269</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6257</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11616</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_1  count\n",
       "0         0  17870\n",
       "1     10357     24\n",
       "2      9379     23\n",
       "3     12030     23\n",
       "4      8818     23\n",
       "5      9547     23\n",
       "6     10385     19\n",
       "7      8269     19\n",
       "8      6257     18\n",
       "9     11616     18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieve the list of clusters of size >=nmax\n",
    "n_max=20\n",
    "large_clusters = clusters_df[f\"cluster_{step-1}\"].value_counts()\n",
    "large_clusters = large_clusters[large_clusters >= n_max].index.tolist()\n",
    "# show cluster_{step-1} value counts top 30, in pandas dataframe\n",
    "large_clusters_df = (\n",
    "    clusters_df[f\"cluster_{step-1}\"].value_counts().reset_index()\n",
    ")\n",
    "large_clusters_df.columns = [f\"cluster_{step-1}\", \"count\"]\n",
    "print(f\"Clusters of size >={n_max}: {len(large_clusters)}\")\n",
    "display(large_clusters_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57c59864",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.vstack(embeddings_df[\"full_name_embedding\"].values)\n",
    "\n",
    "min_cluster_size=3\n",
    "min_samples=3\n",
    "cluster_selection_epsilon=0.0\n",
    "max_cluster_size=30\n",
    "metric=\"euclidean\"\n",
    "n_jobs=-1\n",
    "cluster_selection_method=\"eom\"\n",
    "store_centers=\"medoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ed37fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing large clusters:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-07 23:28:09,230 INFO HDBSCAN completed in 296.01 seconds with min_cluster_size=3, min_samples=3.\n",
      "Processing large clusters:  17%|█▋        | 1/6 [04:56<24:40, 296.05s/it]2025-08-07 23:28:09,280 INFO HDBSCAN completed in 0.03 seconds with min_cluster_size=3, min_samples=3.\n",
      "2025-08-07 23:28:09,310 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=3, min_samples=3.\n",
      "2025-08-07 23:28:09,344 INFO HDBSCAN completed in 0.03 seconds with min_cluster_size=3, min_samples=3.\n",
      "Processing large clusters:  67%|██████▋   | 4/6 [04:56<01:52, 56.04s/it] 2025-08-07 23:28:09,374 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=3, min_samples=3.\n",
      "2025-08-07 23:28:09,403 INFO HDBSCAN completed in 0.02 seconds with min_cluster_size=3, min_samples=3.\n",
      "Processing large clusters: 100%|██████████| 6/6 [04:56<00:00, 49.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# for each large cluster or noise, rerun an hdbscan clustering on the given cluster\n",
    "# create a column 'subcluster' in clusters_df\n",
    "# then create a new column 'cluster_{step}' that is str(cluster_{step-1}) + '_' + str(subcluster)\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step-1}\"]\n",
    "for cluster in tqdm(large_clusters, desc=\"Processing large clusters\"):\n",
    "    # get the indices of the current cluster\n",
    "    indices = clusters_df[clusters_df[f\"cluster_{step-1}\"] == cluster].index.tolist()\n",
    "    # get the embeddings for the current cluster\n",
    "    cluster_embeddings = embeddings[indices]\n",
    "    \n",
    "    # run hdbscan on the current cluster\n",
    "    sub_labels, _ = run_hdbscan(\n",
    "        cluster_embeddings,\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        max_cluster_size=max_cluster_size,\n",
    "        metric=metric,\n",
    "        n_jobs=n_jobs,\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "        store_centers=store_centers,\n",
    "    )\n",
    "    \n",
    "    # update the clusters_df with the new subcluster labels\n",
    "    clusters_df.loc[indices, f\"subcluster_{step}\"] = sub_labels\n",
    "    # compose a list of new cluster ids being str(cluster) + '_' + str(subcluster)\n",
    "    new_labels = str(cluster) + '_' + sub_labels.astype(str)\n",
    "    clusters_df.loc[indices, f\"cluster_{step}\"] = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d35a9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step=2\n",
    "# new_labels = str(cluster) + '_' + sub_labels.astype(str)\n",
    "# # show top 5 labels\n",
    "# print (f\"New labels for step {step}: {new_labels[:5]}\")\n",
    "# clusters_df.loc[indices, f\"cluster_{step}\"] = new_labels\n",
    "# # display(clusters_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d9baf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters in step 2: 12962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>14996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_251</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_489</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_313</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_148</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0_362</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9379_0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0_340</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0_342</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0_343</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_2  count\n",
       "0       0_0  14996\n",
       "1     0_251     25\n",
       "2     0_489     24\n",
       "3     0_313     24\n",
       "4     0_148     24\n",
       "5     0_362     23\n",
       "6    9379_0     23\n",
       "7     0_340     23\n",
       "8     0_342     22\n",
       "9     0_343     22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display top 10 occuring clusters\n",
    "clusters_step_2 = clusters_df[f\"cluster_{step}\"].value_counts().reset_index()\n",
    "clusters_step_2.columns = [f\"cluster_{step}\", \"count\"]\n",
    "print(f\"Number of clusters in step {step}: {len(clusters_step_2)}\")\n",
    "display(clusters_step_2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8bcd58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected cluster is equal to Final_Cluster, except for noise clusters where it is equal to None\n",
    "clusters_df[f\"cluster_{step}\"] = clusters_df[f\"cluster_{step}\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")\n",
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"cluster_{step}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13b230fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clusters table\n",
    "# columns_to_keep = [\n",
    "#     \"object_id\",\n",
    "#     \"full_name\",\n",
    "#     \"number_of_occurences\",\n",
    "#     f\"cluster_{step}\",\n",
    "#]\n",
    "clusters_df.to_excel(clusters_table_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9699c0",
   "metadata": {},
   "source": [
    "## Assess perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d905b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw extracted registry names: 54335\n",
      "Number of Consolidated registry names: 12956\n",
      "Number of un-consolidated/lost (noise) registry names: 15054\n",
      "Percentage of Consolidated registry names: 72.29%\n",
      "Percentage of un-consolidated/lost (noise) registry names: 27.71%\n",
      "---\n",
      "Total number of publications with one of the 'official' extracted registry names: 163102\n",
      "Total number of publications with one of the consolidated registry names: 139836\n",
      "Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): 23266\n",
      "Percentage of lost publications: 14.26%\n"
     ]
    }
   ],
   "source": [
    "# compute % of noise clusters in corrected_cluster\n",
    "noise_clusters = clusters_df[clusters_df[f\"cluster_{step}\"].isna()]\n",
    "noise_percentage = len(noise_clusters) / len(clusters_df) * 100\n",
    "print(f\"Number of raw extracted registry names: {len(clusters_df)}\")\n",
    "# number of consolidated registry names is unique values of corrected_cluster\n",
    "print(f\"Number of Consolidated registry names: {len(clusters_df[f'cluster_{step}'].dropna().unique())}\")\n",
    "print(f\"Number of un-consolidated/lost (noise) registry names: {len(noise_clusters)}\")\n",
    "print(f\"Percentage of Consolidated registry names: {(100-noise_percentage):.2f}%\")\n",
    "print(f\"Percentage of un-consolidated/lost (noise) registry names: {noise_percentage:.2f}%\")\n",
    "print('---')\n",
    "# count total number of occurrences in clusters\n",
    "total_occurrences = clusters_df['number_of_occurrences'].sum()\n",
    "# count total number of occurences of noise clusters\n",
    "total_noise_occurrences = noise_clusters['number_of_occurrences'].sum()\n",
    "print(f\"Total number of publications with one of the 'official' extracted registry names: {total_occurrences}\")\n",
    "print(f\"Total number of publications with one of the consolidated registry names: {total_occurrences - total_noise_occurrences}\")\n",
    "print(f\"Total number of publications lost (with one of the un-consolidated/lost (noise) registry names): {total_noise_occurrences}\")\n",
    "# print %\n",
    "print(f\"Percentage of lost publications: {total_noise_occurrences / total_occurrences * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7558890",
   "metadata": {},
   "source": [
    "### a. Dataset 'Any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33db7641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for any pairs: {'precision': 0.7, 'recall': 0.75, 'f1': 0.73, 'accuracy': 0.79}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions based on cluster mapping\n",
    "eval_df_any = apply_predictions(\n",
    "    eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "# Compute metrics (assuming ground truth is in column \"final_label\")\n",
    "metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_any_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_any.items()\n",
    "}\n",
    "print(f\"Metrics for any pairs: {metrics_any_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_any[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_any_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0822ebf",
   "metadata": {},
   "source": [
    "### b. Dataset 'famous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "245696e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for famous pairs: {'precision': 0.85, 'recall': 0.49, 'f1': 0.62, 'accuracy': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Apply predictions\n",
    "eval_df_famous = apply_predictions(\n",
    "    eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "metrics_famous = compute_metrics(\n",
    "    eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    ")\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_famous_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_famous.items()\n",
    "}\n",
    "print(f\"Metrics for famous pairs: {metrics_famous_to_print}\")\n",
    "# Save predictions Excel file with required columns\n",
    "eval_df_famous[\n",
    "    [\n",
    "        \"full_name\",\n",
    "        \"alias\",\n",
    "        \"number_of_occurrences\",\n",
    "        \"alias_number_of_occurrences\",\n",
    "        \"similarity\",\n",
    "        \"uncertain\",\n",
    "        \"final_label\",\n",
    "        \"prediction\",\n",
    "    ]\n",
    "].to_excel(prediction_famous_results_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a014ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb8bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P05_refine_dedup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
