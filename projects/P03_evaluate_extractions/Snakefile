# Define consolidated dictionary for fields with their associated letters and filenames
FIELDS_DICT = {
    "Medical condition": "medical_condition",
    # "Outcome measure": "outcome_measure",
    "Geographical area": "geographical_area",
    "Registry name": "registry_name",
}

FIELDS_LIST = [
    # "registry_name",
    # "geographical_area",
    # "medical_condition",
    # "outcome_measure",
    "population_description",
    "intervention",
    "comparator",
]

MODELS = [
    "small_mistral",
    # "medium_mistral",
    # "large_mistral",
    # "gpt4o_openai",
    # "gpt4_1_openai",
    # "o3_openai",
]

DATASET_TYPE ="eval" # "test" # 
RAW_PUBLICATIONS_DICT = {
    "eval": "../../datasets/001_publications_dataset/publications_dataset.jsonl",
    "test": "../../datasets/001_publications_dataset/prod_publication_test_dataset.jsonl",
}

# PROMPT_VERSIONS = [
#     "original",
#     "new",
#     # "individual",
# ]
###############################################################################
# 1. Final targets for rule all – **build them explicitly**
###############################################################################

## ---- Workflow 0: Evaluate 'Registry_related' extractions ---- ##
## -- Detail of the W00 targets -- ##
# R00 targets are evaluations of the registry-related extractions, with different models
R00_TARGETS = [
    f"data/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/extraction_results.json"
    for model in MODELS
] + [
    f"data/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/records.jsonl"
    for model in MODELS
]

# R01 targets are the regeneration of the registry-related evaluation dataset
R01_TARGETS = [
    f"data/W00_R01_regenerate_registry_related_eval_dataset/final_eval_dataset.json"
]

# W00 targets are the evaluations of the registry-related extractions, with different models
W00_TARGETS = (
    R00_TARGETS
)

# ---- Workflow 1: Create evaluation datasets for metadata ---- #
# R11 targets are the metadata extractions with both models
R11_TARGETS =[
    f"data/W01/{DATASET_TYPE}/R11_create_extraction_datasets/{field}/{field}_extractions.json"
    for field in FIELDS_LIST
]
# R12 targets are the comparisons of field inferences with matching or llm
R12_TARGETS = [
    f"data/W01/{DATASET_TYPE}/R12_compare_field_inferences_with_matching_or_llm/{field}/compare_{field}_for_{DATASET_TYPE}_dataset_creation.json"
    for field in FIELDS_LIST
]

# use NOTEBOOK dump_new_eval_datasets.ipynb instead of is rule
# R13 targets are the savings of the final evaluation datasets
# R13_TARGETS = [
#     f"data/W01/R13_save_final_evaluation_datasets/{field}/final_eval_dataset.json"
#     for field in FIELDS_LIST
# ]

# W01 targets are the creation of the evaluation datasets for metadata extractions, composed of R11 targets, R12 targets, and R13 targets
W01_TARGETS = (
    R11_TARGETS
    + R12_TARGETS
    # + R13_TARGETS
)


# ---- Workflow 2: Evaluation of metadata extractions ---- #

# R21 targets are the metadata extractions with selected model
R21_TARGETS = [
    f"data/W02/R21_extract_metadata/{model}/metadata_extractions.json"
    for model in MODELS
] + [
    f"data/W02/R21_extract_metadata/{model}/records.jsonl"
    for model in MODELS
]
# R22 targets are the comparisons of metadata extractions with eval dataset
R22_TARGETS = [
    f"data/W02/R22_compare_metadata_extractions_with_eval_dataset/{model}/{field}/compare_{field}_for_final_eval.json"
    for model in MODELS
    for field in FIELDS_LIST
]
# R23 targets are the performance metrics of the metadata extractions with the different models
R23_TARGETS = [
    f"data/W02/R23_compute_and_save_perf_metrics/{model}/{field}/perf_metrics.json"
    for model in MODELS
    for field in FIELDS_LIST
]
# R24 targets are the comparisons of all performance metrics
R24_TARGETS = [
    f"data/W02/R24_compare_all_perf_metrics/perf_metrics.xlsx"
]

# R24_bis targets are the comparisons of all performance metrics and costs
R24_BIS_TARGETS = [
    f"data/W02/R24_bis_compare_all_perf_and_costs/perf_and_costs.xlsx"
]

# W02 targets are the metadata extractions with selected model
W02_TARGETS = (
    R21_TARGETS
    + R22_TARGETS
    + R23_TARGETS
    # + R24_TARGETS
    + R24_BIS_TARGETS
)

# ---- Workflow 3: Evaluation of metadata extractions one by one ---- #
# R31 targets are the metadata extractions with selected model, one by one
R31_TARGETS = [
    f"data/W03/{DATASET_TYPE}/R31_extract_metadata_one_by_one/{field}/{model}/{field}_metadata_extractions.json"
    for model in MODELS
    for field in FIELDS_LIST
] + [
    f"data/W03/{DATASET_TYPE}/R31_extract_metadata_one_by_one/{field}/{model}/{field}_records.jsonl"
    for model in MODELS
    for field in FIELDS_LIST
]

# R32 targets are the comparisons of metadata extractions with eval dataset, one by one
R32_TARGETS = [
    f"data/W03/{DATASET_TYPE}/R32_compare_metadata_extractions_with_{DATASET_TYPE}_dataset_one_by_one/{field}/{model}/compare_{field}_for_final_eval.json"
    for model in MODELS
    for field in FIELDS_LIST
]

# R33 targets are the performance metrics of the metadata extractions with the different models, one by one
R33_TARGETS = [
    f"data/W03/{DATASET_TYPE}/R33_compute_and_save_perf_metrics_one_by_one/{field}/{model}/perf_metrics.json"
    for model in MODELS
    for field in FIELDS_LIST
]

# R34_bis targets are the comparisons of all performance metrics and costs, one by one
R34_BIS_TARGETS = [
    f"data/W03/{DATASET_TYPE}/R34_bis_compare_all_perf_and_costs_one_by_one/perf_and_costs.xlsx"
]

# W03 targets are the metadata extractions with selected model, one by one
W03_TARGETS = (
    R31_TARGETS
    + R32_TARGETS
    + R33_TARGETS
    + R34_BIS_TARGETS
)

# ---- Workflow 99: Sample from prod ---- #
# R99 targets are the sampling from prod
R99_TARGETS = [
    f"data/W99/R99_sample_from_prod/prod_publication_test_dataset.jsonl"
]

# R99_1 targets are the generation of the registry-related test set to annotate
R99_1_TARGETS = [
    f"data/W99/R99_1_generate_registry_related_test_set_to_annotate/final_test_dataset.json"
]



###############################################################################
# 2. rule all – just point to that list
###############################################################################
rule all:
    input:
        # R00_TARGETS,
        # R01_TARGETS,
        # R11_TARGETS,
        R12_TARGETS,
        # R13_TARGETS
        # R21_TARGETS
        # R22_TARGETS,
        # R23_TARGETS,
        # R24_TARGETS
        # R24_BIS_TARGETS,
        # R31_TARGETS,
        # R32_TARGETS,
        # R33_TARGETS,
        # R34_BIS_TARGETS,
        # R99_TARGETS,
        # R99_1_TARGETS,
        # W00_TARGETS,
        # W01_TARGETS,
        # W02_TARGETS,
        # W03_TARGETS,
        

# These rules are now kept as fallbacks, but won't be used unless the "use_existing" rules fail

# -------------------------------------------------------------------------- ######
###### -------- Workflow 0: Evaluate 'Registry_related' extractions -------- ######

# Evaluate performance of extractions from the registry-related extraction pipeline, with the different models, and a selected prompt
rule R00_evaluate_registry_related_entire_pipeline:
    input:
        script="src/scripts/S01_evaluate_registry_related_entire_pipeline.py",
        # base_pubmed_dataset_jsonl="../../datasets/001_publications_dataset/publications_dataset.jsonl",
        base_pubmed_dataset_jsonl=lambda wildcards: RAW_PUBLICATIONS_DICT[DATASET_TYPE],
        prompt_txt="etc/prompts/extraction/registry_related/prompt_extract_properly_registry_related_500.txt",
        # prompt_txt="etc/prompts/extraction/registry_related/prompt_extract_properly_registry_related_original.txt",
        model_config="etc/configs/{model}_config.json",
        eval_dataset_json="../../datasets/005_evaluate_extraction_process_datasets/registry_related/final_{DATASET_TYPE}_dataset.json",
    output:
        excel="data/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/extraction_results.xlsx",
        perf_json="data/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/extraction_results.json",
        records_jsonl="data/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/records.jsonl",
    log:
        "logs/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/registry_related_evaluation.txt",
    shell:
        """
        python {input.script} \\
            --base_pubmed_dataset_jsonl {input.base_pubmed_dataset_jsonl} \\
            --prompt_txt {input.prompt_txt} \\
            --model_config {input.model_config} \\
            --eval_dataset_json {input.eval_dataset_json} \\
            --output_excel {output.excel} \\
            --output_perf_json {output.perf_json} \\
            --output_records_jsonl {output.records_jsonl} \\
            2>&1 | tee {log}
        """

# rule to regenerate/update the registry-related evaluation dataset, using the latest prompt
rule R01_regenerate_registry_related_eval_dataset:
    input:
        script="src/scripts/S02_regenerate_registry_related_eval_dataset.py",
        base_pubmed_dataset_jsonl="../../datasets/001_publications_dataset/publications_dataset.jsonl",
        current_eval_dataset_xlsx="../../datasets/005_evaluate_extraction_process_datasets/registry_related/manual_annotation/01_manually_corrected_registry_related.xlsx",
        prompt_txt="etc/prompts/extraction/registry_related/prompt_extract_properly_registry_related_2000.txt",
        model_a_config="etc/configs/gpt4_1_openai_config.json",
        model_b_config="etc/configs/large_mistral_config.json",
    output:
        json="data/W00_R01_regenerate_registry_related_eval_dataset/final_eval_dataset.json",
    log:
        "logs/W00_R01_regenerate_registry_related_eval_dataset/update_final_eval_dataset.txt",
    shell:
        """
        python {input.script} \\
            --base_pubmed_dataset_jsonl {input.base_pubmed_dataset_jsonl} \\
            --current_eval_dataset_xlsx {input.current_eval_dataset_xlsx} \\
            --prompt_txt {input.prompt_txt} \\
            --model_a_config {input.model_a_config} \\
            --model_b_config {input.model_b_config} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """

# -------------------------------------------------------------------------- ######
###### -------- Workflow 1: Create evaluation datasets for metadata -------- ######

# Make metadata extractions with small then large mistral, and save both in the same file
rule R11_metadata_extraction_small_and_large_mistral:
    input:
        script="src/scripts/S11_extract_metadata_with_both_models.py",
        # base_pubmed_dataset_jsonl="../../datasets/001_publications_dataset/publications_dataset.jsonl",
        # base_pubmed_dataset_jsonl="../../datasets/001_publications_dataset/prod_publication_test_dataset.jsonl",
        base_pubmed_dataset_jsonl=lambda wildcards: RAW_PUBLICATIONS_DICT[DATASET_TYPE],
        registry_related_dataset_json="../../datasets/005_evaluate_extraction_process_datasets/registry_related/final_{DATASET_TYPE}_dataset.json",
        model_a_config="etc/configs/small_mistral_config.json",
        model_b_config="etc/configs/large_mistral_config.json",
        # prompt_txt="etc/prompts/extraction/metadata/prompt_publications_metadata_extraction_evaluation.txt",
        prompt_txt="etc/prompts/extraction/metadata/individual_prompts/prompt_{field}.txt",
        # prompt_txt="etc/prompts/extraction/metadata/individual_prompts/prompt_other_fields.txt",
    output:
        json="data/W01/{DATASET_TYPE}/R11_create_extraction_datasets/{field}/{field}_extractions.json",
    log:
        "logs/W01/{DATASET_TYPE}/R11_create_extraction_datasets/{DATASET_TYPE}/{field}/create_{field}_extractions.txt",
    shell:
        """
        python {input.script} \\
            --base_pubmed_dataset_jsonl {input.base_pubmed_dataset_jsonl} \\
            --registry_related_dataset_json {input.registry_related_dataset_json} \\
            --model_a_config {input.model_a_config} \\
            --model_b_config {input.model_b_config} \\
            --prompt_txt {input.prompt_txt} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """

# Load new inferences, load the annotations that were already made to avoid uselss llm_as_a_judge inferences
# For the ones without annotations, :
    # If, both model lower case inferences match perfectly, then correct_inference = model_b
    # else if only one model has 'Not specified' in the field (and not the other model), then we need new manual annotation -> empty correct_inference
    # else: use llm_as_a_judge to compare the two models inferences. if 'same', then correct_inference = model_b, else need for new manual annotation -> empty correct_inference
# save all the samples (even the ones that were already annotated) in a new json file. Some of them will have correct_inference = None
rule R12_compare_field_inferences_with_matching_or_llm:
    input:
        script="src/scripts/S12_compare_field_inferences_with_matching_or_llm.py",
        # previous_annotations="../../datasets/005_evaluate_extraction_process_datasets/{field}/manual_annotation/01_manually_corrected_{field}.xlsx",
        metadata_extractions="data/W01/{DATASET_TYPE}/R11_create_extraction_datasets/{field}/{field}_extractions.json",
        model_config="etc/configs/gpt4_1_openai_config.json",
        # model_config="etc/configs/medium_mistral_config.json",
        prompt_txt="etc/prompts/llm_as_a_judge_annotation/compare_{field}_for_eval_dataset_creation.txt",
    params:
        field="{field}",
    output:
        json="data/W01/{DATASET_TYPE}/R12_compare_field_inferences_with_matching_or_llm/{field}/compare_{field}_for_{DATASET_TYPE}_dataset_creation.json",
    log:
        "logs/W01/{DATASET_TYPE}/R12_compare_field_inferences_with_matching_or_llm/{field}/compare_{field}_for_{DATASET_TYPE}_dataset_creation.txt",
    shell:
        """
        python {input.script} \\
            --metadata_extractions {input.metadata_extractions} \\
            --model_config {input.model_config} \\
            --prompt_txt {input.prompt_txt} \\
            --field {params.field} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """

# After R12, dataset with the publciations left to annotated is shared to the team to be annotated
# then the new version of the dataset is saved in the location ../../datasets/005_evaluate_extraction_process_datasets/{field}/manual_annotation/01_manually_corrected_{field}.xlsx
# dataset "data/W01/R12_compare_field_inferences_with_matching_or_llm/{field}/compare_{field}_for_eval_dataset_creation.json" join left with the new dataset
# saved to "data/W01/R13_save_final_evaluation_datasets/{field}/final_eval_dataset.json" -> later manually saved and dvc pushed to ../../datasets/005_evaluate_extraction_process_datasets/A_medical_condition/medical_condition_eval_dataset.json
# rule R13_save_final_evaluation_dataset:
#     input:
#         script="src/scripts/S13_save_final_evaluation_dataset.py",
#         publications_already_annotated="data/W01/R12_compare_field_inferences_with_matching_or_llm/{field}/compare_{field}_for_eval_dataset_creation.json",
#         publications_newly_annotated="../../datasets/005_evaluate_extraction_process_datasets/{field}/manual_annotation/01_manually_corrected_{field}.xlsx",
#     params:
#         field="{field}",
#     output:
#         json="data/W01/R13_save_final_evaluation_datasets/{field}/final_eval_dataset.json",
#     log:    
#         "logs/W01/R13_save_final_evaluation_datasets/{field}/save_final_evaluation_dataset.txt",
#     shell:
#         """
#         python {input.script} \\
#             --publications_already_annotated {input.publications_already_annotated} \\
#             --publications_newly_annotated {input.publications_newly_annotated} \\
#             --field {params.field} \\
#             --output_json {output.json} \\
#             2>&1 | tee {log}
#         """


# -------------------------------------------------------------------------- ######
###### --------    Workflow 2: Evaluation of metadata extractions   -------- ######

# extract metadata with selected model
rule R21_extract_metadata:
    input:
        script="src/scripts/S21_extract_metadata.py",
        base_pubmed_dataset_jsonl="../../datasets/001_publications_dataset/publications_dataset.jsonl",
        registry_related_eval_dataset_json="../../datasets/005_evaluate_extraction_process_datasets/registry_related/final_eval_dataset.json",
        model_config="etc/configs/{model}_config.json",
        # prompt_txt="etc/prompts/extraction/metadata/prompt_publications_metadata_extraction_evaluation_new.txt",
        prompt_txt="etc/prompts/extraction/metadata/prompt_publications_metadata_extraction_evaluation_original.txt",
    params:
        model="{model}",
    output:
        json="data/W02/R21_extract_metadata/{model}/metadata_extractions.json",
        records_jsonl="data/W02/R21_extract_metadata/{model}/records.jsonl",
    log:
        "logs/W02/R21_extract_metadata/{model}/extract_metadata.txt",
    shell:
        """
        python {input.script} \\
            --base_pubmed_dataset_jsonl {input.base_pubmed_dataset_jsonl} \\
            --registry_related_eval_dataset_json {input.registry_related_eval_dataset_json} \\
            --model_config {input.model_config} \\
            --prompt_txt {input.prompt_txt} \\
            --model {params.model} \\
            --output_json {output.json} \\
            --output_records_jsonl {output.records_jsonl} \\
            2>&1 | tee {log}
        """

# Compare the new metadata extractions with the ones from the evaluation dataset. one rule per model and per field
rule R22_compare_metadata_extractions_with_eval_dataset:
    input:
        script="src/scripts/S22_compare_metadata_extractions_with_eval_dataset.py",
        metadata_extractions="data/W02/R21_extract_metadata/{model}/metadata_extractions.json",
        eval_dataset_json="../../datasets/005_evaluate_extraction_process_datasets/{field}/final_eval_dataset.json",
        # eval_dataset_json="data/W01/R13_save_final_evaluation_datasets/{field}/final_eval_dataset.json",
        # llm_judge_model_config="etc/configs/gpt4o_openai_config.json",
        llm_judge_model_config="etc/configs/gpt4_1_openai_config.json",
        prompt_llm_judge = "etc/prompts/llm_as_a_judge_final_assessment/compare_{field}_for_final_eval.txt",
    params:
        model="{model}",
        field="{field}",
    output:
        json="data/W02/R22_compare_metadata_extractions_with_eval_dataset/{model}/{field}/compare_{field}_for_final_eval.json",
    log:
        "logs/W02/R22_compare_metadata_extractions_with_eval_dataset/{model}/{field}/compare_{field}_for_final_eval.txt",
    shell:
        """
        python {input.script} \\
            --metadata_extractions {input.metadata_extractions} \\
            --eval_dataset_json {input.eval_dataset_json} \\
            --llm_judge_model_config {input.llm_judge_model_config} \\
            --prompt_llm_judge {input.prompt_llm_judge} \\
            --model {params.model} \\
            --field {params.field} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """

# R23 targets are the performance metrics of the metadata extractions with the different models
rule R23_compute_and_save_perf_metrics:
    input:
        script="src/scripts/S23_compute_and_save_perf_metrics.py",
        extraction_results="data/W02/R22_compare_metadata_extractions_with_eval_dataset/{model}/{field}/compare_{field}_for_final_eval.json",
        # eval_dataset_json="data/W01/R13_save_final_evaluation_datasets/{field}/final_eval_dataset.json",
    params:
        model="{model}",
        field="{field}",
    output:
        json="data/W02/R23_compute_and_save_perf_metrics/{model}/{field}/perf_metrics.json",
    log:
        "logs/W02/R23_compute_and_save_perf_metrics/{model}/{field}/perf_metrics.txt",
    shell:
        """
        python {input.script} \\
            --extraction_results {input.extraction_results} \\
            --model {params.model} \\
            --field {params.field} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """

# regroup in a table all perfs metrics, all models and all fields (including registry_related from W00). Save as a excel table
rule R24_compare_all_perf_metrics:
    input:
        script="src/scripts/S24_compare_all_perf_metrics.py",
        perf_metrics_jsons=expand(
            "data/W02/R23_compute_and_save_perf_metrics/{model}/{field}/perf_metrics.json",
            model=MODELS,
            field=FIELDS_LIST,
        ),
        registry_related_perf_jsons=expand(
            "data/W00_R00_evaluate_registry_related_entire_pipeline/{model}/extraction_results.json",
            model=MODELS,
        ),
    output:
        excel="data/W02/R24_compare_all_perf_metrics/perf_metrics.xlsx",
    log:
        "logs/W02/R24_compare_all_perf_metrics/compare_all_perf_metrics.txt",
    shell:
        """
        python {input.script} \\
            $(for f in {input.perf_metrics_jsons}; do echo "--perf_metrics_jsons $f"; done) \\
            $(for f in {input.registry_related_perf_jsons}; do echo "--registry_related_perf_jsons $f"; done) \\
            --output_excel {output.excel} \\
            2>&1 | tee {log}
        """

# regroup in a table all perfs metrics & costs, all models and all fields (including registry_related from W00). Save as a excel table
rule R24_bis_compare_all_perf_and_costs:
    input:
        script="src/scripts/S24_bis_compare_all_perf_and_costs.py",
        perf_metrics_jsons=expand(
            "data/W02/R23_compute_and_save_perf_metrics/{model}/{field}/perf_metrics.json",
            model=MODELS,
            field=FIELDS_LIST,
        ),
        llm_records_jsonls=expand(
            "data/W02/R21_extract_metadata/{model}/records.jsonl",
            model=MODELS,
        ),
        registry_related_perf_jsons=expand(
            "data/W00_R00_evaluate_registry_related_entire_pipeline/{model}/extraction_results.json",
            model=MODELS,
        ),
        llm_records_jsonls_registry_related=expand(
            "data/W00_R00_evaluate_registry_related_entire_pipeline/{model}/records.jsonl",
            model=MODELS,
        ),
    output:
        excel="data/W02/R24_bis_compare_all_perf_and_costs/perf_and_costs.xlsx",
    log:
        "logs/W02/R24_compare_all_perf_metrics/compare_all_perf_and_costs.txt",
    shell:
        """
        python {input.script} \\
            $(for f in {input.perf_metrics_jsons}; do echo "--perf_metrics_jsons $f"; done) \\
            $(for f in {input.llm_records_jsonls}; do echo "--llm_records_jsonls $f"; done) \\
            $(for f in {input.registry_related_perf_jsons}; do echo "--registry_related_perf_jsons $f"; done) \\
            $(for f in {input.llm_records_jsonls_registry_related}; do echo "--llm_records_jsonls_registry_related $f"; done) \\
            --output_excel {output.excel} \\
            2>&1 | tee {log}
        """


#####################################################################################
# same pipeline as above, but with by extracting each field one by one

rule R31_extract_metadata_one_by_one:
    input:
        script="src/scripts/S31_extract_metadata_one_by_one.py",
        base_pubmed_dataset_jsonl=lambda wildcards: RAW_PUBLICATIONS_DICT[DATASET_TYPE],
        registry_related_eval_dataset_json="../../datasets/005_evaluate_extraction_process_datasets/registry_related/final_{DATASET_TYPE}_dataset.json",
        model_config="etc/configs/{model}_config.json",
        prompt_field="etc/prompts/extraction/metadata/individual_prompts/prompt_{field}.txt",
    params:
        model="{model}",
        field="{field}",
    output:
        field_json="data/W03/{DATASET_TYPE}/R31_extract_metadata_one_by_one/{field}/{model}/{field}_metadata_extractions.json",
        field_records_jsonl="data/W03/{DATASET_TYPE}/R31_extract_metadata_one_by_one/{field}/{model}/{field}_records.jsonl",
    log:
        "logs/W03/{DATASET_TYPE}/R31_extract_metadata_one_by_one/{field}/{model}/{field}_extract_metadata.txt",
    shell:
        """
        python {input.script} \\
            --base_pubmed_dataset_jsonl {input.base_pubmed_dataset_jsonl} \\
            --registry_related_eval_dataset_json {input.registry_related_eval_dataset_json} \\
            --model_config {input.model_config} \\
            --prompt_field {input.prompt_field} \\
            --model {params.model} \\
            --field {params.field} \\
            --output_field_json {output.field_json} \\
            --output_field_records_jsonl {output.field_records_jsonl} \\
            2>&1 | tee {log}
        """

# Compare the new metadata extractions with the ones from the evaluation dataset. one rule per model and per field
rule R32_compare_metadata_extractions_with_eval_dataset_one_by_one:
    input:
        script="src/scripts/S32_compare_metadata_extractions_with_eval_dataset.py",
        metadata_extractions="data/W03/{DATASET_TYPE}/R31_extract_metadata_one_by_one/{field}/{model}/{field}_metadata_extractions.json",
        eval_dataset_json="../../datasets/005_evaluate_extraction_process_datasets/{field}/final_{DATASET_TYPE}_dataset.json",
        llm_judge_model_config="etc/configs/gpt4_1_openai_config.json",
        prompt_llm_judge="etc/prompts/llm_as_a_judge_final_assessment/compare_{field}_for_final_eval.txt",
    params:
        model="{model}",
        field="{field}",
    output:
        json="data/W03/{DATASET_TYPE}/R32_compare_metadata_extractions_with_{DATASET_TYPE}_dataset_one_by_one/{field}/{model}/compare_{field}_for_final_eval.json",
    log:
        "logs/W03/{DATASET_TYPE}/R32_compare_metadata_extractions_with_{DATASET_TYPE}_dataset_one_by_one/{field}/{model}/compare_{field}_for_final_eval.txt",
    shell:
        """
        python {input.script} \\
            --metadata_extractions {input.metadata_extractions} \\
            --eval_dataset_json {input.eval_dataset_json} \\
            --llm_judge_model_config {input.llm_judge_model_config} \\
            --prompt_llm_judge {input.prompt_llm_judge} \\
            --model {params.model} \\
            --field {params.field} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """

# R33 targets are the performance metrics of the metadata extractions with the different models
rule R33_compute_and_save_perf_metrics_one_by_one:
    input:
        script="src/scripts/S33_compute_and_save_perf_metrics.py",
        extraction_results="data/W03/{DATASET_TYPE}/R32_compare_metadata_extractions_with_eval_dataset_one_by_one/{field}/{model}/compare_{field}_for_final_eval.json",
    params:
        model="{model}",
        field="{field}",
    output:
        json="data/W03/{DATASET_TYPE}/R33_compute_and_save_perf_metrics_one_by_one/{field}/{model}/perf_metrics.json",
    log:
        "logs/W03/{DATASET_TYPE}/R33_compute_and_save_perf_metrics_one_by_one/{field}/{model}/perf_metrics.txt",
    shell:
        """
        python {input.script} \\
            --extraction_results {input.extraction_results} \\
            --model {params.model} \\
            --field {params.field} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """

# # regroup in a table all perfs metrics and costs, all models and all fields (including registry_related from W00). Save as a excel table. Equivalent of R24_bis
# rule R34_bis_compare_all_perf_and_costs_one_by_one:
#     input:
#         script="src/scripts/S34_bis_compare_all_perf_and_costs.py",
#         perf_metrics_jsons=expand(
#             "data/W03/{DATASET_TYPE}/R33_compute_and_save_perf_metrics_one_by_one/{field}/{model}/perf_metrics.json",
#             model=MODELS,
#             field=FIELDS_LIST,
#         ),
#         llm_records_jsonls=expand(
#             "data/W03/{DATASET_TYPE}/R31_extract_metadata_one_by_one/{field}/{model}/{field}_records.jsonl",
#             model=MODELS,
#             field=FIELDS_LIST,
#         ),
#         registry_related_perf_jsons=expand(
#             "data/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/extraction_results.json",
#             model=MODELS,
#         ),
#         llm_records_jsonls_registry_related=expand(
#             "data/W00/{DATASET_TYPE}/R00_evaluate_registry_related_entire_pipeline/{model}/records.jsonl",
#             model=MODELS,
#         ),
#     output:
#         excel="data/W03/{DATASET_TYPE}/R34_bis_compare_all_perf_and_costs_one_by_one/perf_and_costs.xlsx",
#     log:
#         "logs/W03/{DATASET_TYPE}/R34_bis_compare_all_perf_and_costs_one_by_one/compare_all_perf_and_costs.txt",
#     shell:
#         """
#         python {input.script} \\
#             $(for f in {input.perf_metrics_jsons}; do echo "--perf_metrics_jsons $f"; done) \\
#             $(for f in {input.llm_records_jsonls}; do echo "--llm_records_jsonls $f"; done) \\
#             $(for f in {input.registry_related_perf_jsons}; do echo "--registry_related_perf_jsons $f"; done) \\
#             $(for f in {input.llm_records_jsonls_registry_related}; do echo "--llm_records_jsonls_registry_related $f"; done) \\
#             --output_excel {output.excel} \\
#             2>&1 | tee {log}
#         """

# rule R99 is to sample publications from prod
rule R99_sample_from_prod:
    input:
        script="src/scripts/S99_sample_from_prod.py"
    output:
        jsonl="data/W99/R99_sample_from_prod/prod_publication_test_dataset.jsonl"
    params:
        n_samples=100,
        collection_name="Publication_v2"
    shell:
        """
        python {input.script} \
          --n_samples {params.n_samples} \
          --collection_name {params.collection_name} \
          --output_jsonl {output.jsonl}
        """


# rule to regenerate/update the registry-related evaluation dataset, using the latest prompt
rule R99_1_generate_registry_related_test_set_to_annotate:
    input:
        script="src/scripts/S99_1_generate_registry_related_test_set_to_annotate.py",
        base_pubmed_dataset_jsonl="../../datasets/001_publications_dataset/prod_publication_test_dataset.jsonl",
        # current_eval_dataset_xlsx="../../datasets/005_evaluate_extraction_process_datasets/registry_related/manual_annotation/01_manually_corrected_registry_related.xlsx",
        prompt_txt="etc/prompts/extraction/registry_related/prompt_extract_properly_registry_related_2000.txt",
        model_a_config="etc/configs/large_mistral_config.json",
        model_b_config="etc/configs/medium_magistral_config.json",
    output:
        json="data/W99/R99_1_generate_registry_related_test_set_to_annotate/final_test_dataset.json",
    log:
        "logs/W99/R99_1_generate_registry_related_test_set_to_annotate/update_final_test_dataset.txt",
    shell:
        """
        python {input.script} \\
            --base_pubmed_dataset_jsonl {input.base_pubmed_dataset_jsonl} \\
            --prompt_txt {input.prompt_txt} \\
            --model_a_config {input.model_a_config} \\
            --model_b_config {input.model_b_config} \\
            --output_json {output.json} \\
            2>&1 | tee {log}
        """


