{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa75539",
   "metadata": {},
   "source": [
    "# Grid Search for Best Epsilon Configuration\n",
    "This notebook performs a grid search over multiple epsilon values for HDBSCAN clustering, evaluating each configuration on two datasets and saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2119f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.pyenv/versions/3.11.11/envs/P05_refine_dedup_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Set working directory (adjust if needed)\n",
    "working_dir = '/home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup'\n",
    "os.chdir(working_dir)\n",
    "print(f'Changed working directory to {working_dir}')\n",
    "from src.p05_refine_dedup import config\n",
    "from src.p05_refine_dedup.utils.utils import (\n",
    "    is_noise,\n",
    "    run_hdbscan,\n",
    "    apply_predictions,\n",
    "    compute_metrics,\n",
    ")\n",
    "from src.p05_refine_dedup.utils.s3_io_functions import (\n",
    "    load_parquet_from_s3,\n",
    ")\n",
    "\n",
    "output_dir = Path(\"data/W03/from_notebooks/R06_additional_grid_search/v5\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_xlsx = output_dir / 'grid_search_results.xlsx'\n",
    "best_config_json = output_dir / 'best_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cabcf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data will be loaded from s3://s3-common-dev20231214174437248800000002/registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "s3_input_embeddings = 'registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet'\n",
    "bucket_name = config.BUCKET_NAME_DEV\n",
    "folder_path = s3_input_embeddings.rsplit('/', 1)[0]\n",
    "file_name = s3_input_embeddings.rsplit('/', 1)[-1]\n",
    "embeddings_df = load_parquet_from_s3(\n",
    "    bucket_name=bucket_name,\n",
    "    folder_path=folder_path,\n",
    "    file_name=file_name,\n",
    ")\n",
    "clusters_table_xlsx = 'data/W02/R02_evaluate_model_performance/clusters_table.xlsx'\n",
    "clusters_df = pd.read_excel(clusters_table_xlsx)\n",
    "clusters_df = clusters_df.merge(\n",
    "    embeddings_df[['full_name', 'full_name_embedding']],\n",
    "    on='full_name',\n",
    "    how='left'\n",
    ")\n",
    "clusters_df.rename(columns={'Final_Cluster': 'cluster_0'}, inplace=True)\n",
    "\n",
    "evaluation_dataset_any = 'data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/assessed_pairs_v1.xlsx'\n",
    "evaluation_dataset_famous = 'data/W01/R03_eval_pairs_similarity_assessment_with_llm/gpt4_1_openai/famous_close_assessed_pairs_v1.xlsx'\n",
    "eval_df_any = pd.read_excel(evaluation_dataset_any)\n",
    "eval_df_famous = pd.read_excel(evaluation_dataset_famous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27b9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epsilon ranges for each cluster type\n",
    "eps_grid = {\n",
    "    '0_0': np.arange(0.44, 0.51, 0.01),\n",
    "    'start_0_': np.arange(0.34, 0.44, 0.01),\n",
    "    'end__0': np.arange(0.28, 0.33, 0.01),\n",
    "    'other': np.arange(0.19, 0.27, 0.01),\n",
    "}\n",
    "# convert array to list of str()\n",
    "for key in eps_grid:\n",
    "    eps_grid[key] = [str(round(x, 2)) for x in eps_grid[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60568134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to get epsilon for a cluster given the current config\n",
    "def get_epsilon(cluster_id, eps_config):\n",
    "    if cluster_id == '0_0':\n",
    "        return eps_config['0_0']\n",
    "    elif cluster_id.startswith('0_'):\n",
    "        return eps_config['start_0_']\n",
    "    elif cluster_id.endswith('_0'):\n",
    "        return eps_config['end__0']\n",
    "    else:\n",
    "        return eps_config['other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2773674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one dictionary of new cluseters ids for each cluster type\n",
    "# dict_0_0 = {\n",
    "#     0.44:[\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '0',\n",
    "#          'cluster_1': '0_0_0'\n",
    "#          },\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '1',\n",
    "#          'cluster_1': '0_0_1'\n",
    "#          },\n",
    "#          {'full_name': 'name_2',\n",
    "#           'cluster_0': '0_0',\n",
    "#           'subcluster': '2',\n",
    "#           'cluster_1': '0_0_2'\n",
    "#           }\n",
    "#           ...\n",
    "#          ],\n",
    "#     0.45:[\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '0',\n",
    "#          'cluster_1': '0_0_0'\n",
    "#          },\n",
    "#         {'full_name': 'name_1',\n",
    "#          'cluster_0': '0_0',\n",
    "#          'subcluster': '1',\n",
    "#          'cluster_1': '0_0_1'\n",
    "#          },\n",
    "#          {'full_name': 'name_2',\n",
    "#           'cluster_0': '0_0',\n",
    "#           'subcluster': '2',\n",
    "#           'cluster_1': '0_0_2'\n",
    "#           }\n",
    "#           ...\n",
    "#          ],\n",
    "#     ...\n",
    "# }\n",
    "def filter_on_cluster_type(clusters_df, cluster_type):\n",
    "    if cluster_type == '0_0':\n",
    "        return clusters_df[clusters_df['cluster_0'] == cluster_type]\n",
    "    elif cluster_type == 'start_0_':\n",
    "        # starting with '0_' and not ending with '_0'\n",
    "        return clusters_df[(clusters_df['cluster_0'].str.startswith('0_')) &\n",
    "                           (~clusters_df['cluster_0'].str.endswith('_0'))]\n",
    "    elif cluster_type == 'end__0':\n",
    "        # ending with '_0' and not starting with '0_'\n",
    "        return clusters_df[clusters_df['cluster_0'].str.endswith('_0') &\n",
    "                           (~clusters_df['cluster_0'].str.startswith('0_'))]\n",
    "    elif cluster_type == 'other':\n",
    "        return clusters_df[~clusters_df['cluster_0'].str.startswith('0_') &\n",
    "                           ~clusters_df['cluster_0'].str.endswith('_0')]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown cluster type: {cluster_type}\")\n",
    "\n",
    "# Initiate dict_0_0 with all espilons and full_name and cluster_0 only\n",
    "def create_initial_dict(clusters_df, cluster_type):\n",
    "    initial_dict = {}\n",
    "    for eps in eps_grid[cluster_type]:\n",
    "        clusters_df_filtered = filter_on_cluster_type(clusters_df, cluster_type)\n",
    "        initial_dict[eps] = clusters_df_filtered[['full_name', 'cluster_0']].copy()\n",
    "        # set subcluster and cluster_1 to None\n",
    "        initial_dict[eps]['subcluster'] = None\n",
    "        initial_dict[eps]['cluster_1'] = None\n",
    "    return initial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5de6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 20\n",
    "large_clusters = clusters_df['cluster_0'].value_counts()[clusters_df['cluster_0'].value_counts() >= n_max].index.tolist()\n",
    "# filter on large clusters\n",
    "large_clusters_df = clusters_df[clusters_df['cluster_0'].isin(large_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27465cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate dictionaries for each cluster type\n",
    "dict_0_0 = create_initial_dict(large_clusters_df, '0_0')\n",
    "dict_start_0_ = create_initial_dict(large_clusters_df, 'start_0_')\n",
    "dict_end__0 = create_initial_dict(large_clusters_df, 'end__0')\n",
    "dict_other = create_initial_dict(large_clusters_df, 'other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fd6d7",
   "metadata": {},
   "source": [
    "# prepare dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d3ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=2\n",
    "min_samples=2\n",
    "# cluster_selection_epsilon=0.0\n",
    "max_cluster_size=20\n",
    "metric=\"euclidean\"\n",
    "n_jobs=-1\n",
    "cluster_selection_method=\"eom\"\n",
    "store_centers=\"medoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af310ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_clusters(clusters_df, cluster_type, results_dict):\n",
    "    # first filter on cluster type\n",
    "    clusters_df_filtered = filter_on_cluster_type(clusters_df, cluster_type)\n",
    "    # # test on 100 data points\n",
    "    # clusters_df_filtered = clusters_df_filtered.head(1000) # For testing, remove this line for full dataset\n",
    "    # retrive the list of clusters in cluster_0\n",
    "    clusters = clusters_df_filtered['cluster_0'].unique().tolist()\n",
    "\n",
    "    for eps in tqdm(eps_grid[cluster_type], desc=f\"Processing {cluster_type} clusters\"):\n",
    "        for cluster in clusters:\n",
    "            # first filter on this cluster\n",
    "            df = clusters_df_filtered[clusters_df_filtered['cluster_0'] == cluster].copy()\n",
    "            embeddings = np.vstack(df['full_name_embedding'].values)\n",
    "\n",
    "            # Add subcluster and cluster_1 columns\n",
    "            df['subcluster'] = None\n",
    "            df['cluster_1'] = None\n",
    "            \n",
    "            # Apply HDBSCAN clustering\n",
    "            labels, comp_time = run_hdbscan(\n",
    "                embeddings,\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                cluster_selection_epsilon=float(eps),\n",
    "                max_cluster_size=max_cluster_size,\n",
    "                metric=metric,\n",
    "                n_jobs=n_jobs,\n",
    "                cluster_selection_method=cluster_selection_method,\n",
    "                store_centers=store_centers,\n",
    "            )\n",
    "            \n",
    "            df['subcluster'] = labels.astype(str)  # Convert labels to string for subcluster\n",
    "            df['cluster_1'] = df['cluster_0'] + '_' + df['subcluster']\n",
    "\n",
    "            # Vectorized update: set full_name as index for both DataFrames, then update\n",
    "            updates = df[['full_name', 'subcluster', 'cluster_1']].set_index('full_name')\n",
    "            res_df = results_dict[eps].set_index('full_name')\n",
    "            res_df.update(updates)\n",
    "            results_dict[eps] = res_df.reset_index()\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "569c6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test on 100 data points on '0_0' clusters\n",
    "# dict_0_0 = process_clusters(large_clusters_df, '0_0', dict_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afb1bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing start_0_ clusters:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing start_0_ clusters: 100%|██████████| 10/10 [04:17<00:00, 25.74s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_start_0_ = process_clusters(large_clusters_df, 'start_0_', dict_start_0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6286c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing end__0 clusters: 100%|██████████| 5/5 [00:06<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_end__0 = process_clusters(large_clusters_df, 'end__0', dict_end__0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07d9c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing other clusters:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing other clusters: 100%|██████████| 9/9 [00:24<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_other = process_clusters(large_clusters_df, 'other', dict_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4238166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each dictionary to a json file dict_0_0, dict_start_0_, dict_end__0, dict_other\n",
    "# not looping on eps values, save direclty all eps values in on single json file\n",
    "def save_dict_to_json(data_dict, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data_dict, f, indent=4)\n",
    "# # transform each object (dataframe) in the dict to a dict\n",
    "# dict_0_0 = {eps: df.to_dict(orient='records') for eps, df in dict_0_0.items()}\n",
    "dict_start_0_ = {eps: df.to_dict(orient='records') for eps, df in dict_start_0_.items()}\n",
    "dict_end__0 = {eps: df.to_dict(orient='records') for eps, df in dict_end__0.items()}\n",
    "dict_other = {eps: df.to_dict(orient='records') for eps, df in dict_other.items()}\n",
    "\n",
    "# Save dictionaries to json files\n",
    "# save_dict_to_json(dict_0_0, output_dir / 'dict_0_0.json')\n",
    "save_dict_to_json(dict_start_0_, output_dir / 'dict_start_0_.json')\n",
    "# save_dict_to_json(dict_end__0, output_dir / 'dict_end__0.json')\n",
    "save_dict_to_json(dict_other, output_dir / 'dict_other.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b204fb",
   "metadata": {},
   "source": [
    "# compute original metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd02775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for any pairs: {'precision': 0.72, 'recall': 0.79, 'f1': 0.76, 'accuracy': 0.81}\n",
      "Metrics for famous pairs: {'precision': 0.81, 'recall': 0.77, 'f1': 0.79, 'accuracy': 0.86}\n"
     ]
    }
   ],
   "source": [
    "clusters_df[f\"corrected_cluster\"] = clusters_df[f\"corrected_cluster\"].apply(\n",
    "    lambda x: None if is_noise(x) else x\n",
    ")\n",
    "# recompute current performance metrics\n",
    "cluster_map = dict(zip(clusters_df[\"full_name\"], clusters_df[f\"corrected_cluster\"]))\n",
    "# Apply predictions based on cluster mapping\n",
    "eval_df_any = apply_predictions(\n",
    "    eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "# Compute metrics (assuming ground truth is in column \"final_label\")\n",
    "metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_any_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_any.items()\n",
    "}\n",
    "print(f\"Metrics for any pairs: {metrics_any_to_print}\")\n",
    "\n",
    "# Apply predictions\n",
    "eval_df_famous = apply_predictions(\n",
    "    eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    ")\n",
    "metrics_famous = compute_metrics(\n",
    "    eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    ")\n",
    "# log the metrics with 2 decimal precision\n",
    "metrics_famous_to_print = {\n",
    "    k: round(v, 2) if isinstance(v, float) else v for k, v in metrics_famous.items()\n",
    "}\n",
    "print(f\"Metrics for famous pairs: {metrics_famous_to_print}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc62fc5",
   "metadata": {},
   "source": [
    "# reload an compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e37203c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the dictionaries from json files\n",
    "def load_dict_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "output_dir_original = Path(\"data/W03/from_notebooks/R06_additional_grid_search/v5\")\n",
    "# dict_0_0 = load_dict_from_json(output_dir_original / 'dict_0_0.json')\n",
    "dict_start_0_ = load_dict_from_json(output_dir_original / 'dict_start_0_.json')\n",
    "# dict_end__0 = load_dict_from_json(output_dir_original / 'dict_end__0.json')\n",
    "dict_other = load_dict_from_json(output_dir_original / 'dict_other.json')\n",
    "\n",
    "# transform each object (dict) in the dict to a dataframe\n",
    "def transform_dict_to_df(data_dict):\n",
    "    return {eps: pd.DataFrame(records) for eps, records in data_dict.items()}\n",
    "# dict_0_0 = transform_dict_to_df(dict_0_0)\n",
    "dict_start_0_ = transform_dict_to_df(dict_start_0_)\n",
    "# dict_end__0 = transform_dict_to_df(dict_end__0)\n",
    "dict_other = transform_dict_to_df(dict_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a56624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2008/2330050107.py:122: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  grid_search_results = pd.concat(\n",
      "Processing epsilon combinations:   1%|          | 1/90 [00:00<00:48,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best precision_mean: 0.8 with eps: ('0.34', '0.19')\n",
      "--- New best f1_mean: 0.75 with eps: ('0.34', '0.19')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   2%|▏         | 2/90 [00:01<00:48,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.76 with eps: ('0.34', '0.2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   3%|▎         | 3/90 [00:01<00:47,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.76 with eps: ('0.34', '0.21')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   4%|▍         | 4/90 [00:02<00:47,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.77 with eps: ('0.34', '0.22')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   6%|▌         | 5/90 [00:02<00:46,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.77 with eps: ('0.34', '0.23')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   7%|▋         | 6/90 [00:03<00:46,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.77 with eps: ('0.34', '0.24')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   8%|▊         | 7/90 [00:03<00:45,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.78 with eps: ('0.34', '0.25')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations:   9%|▉         | 8/90 [00:04<00:44,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New best f1_mean: 0.78 with eps: ('0.34', '0.26')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epsilon combinations: 100%|██████████| 90/90 [00:48<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration: {'eps_start_0_': 0.34, 'eps_other': 0.26, 'f1_mean': 0.78, 'precision_mean': 0.79, 'recall_mean': 0.77, 'f1_any': 0.77, 'precision_any': 0.74, 'recall_any': 0.79, 'f1_famous': 0.79, 'precision_famous': 0.83, 'recall_famous': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initiate grid_search_results with columns eps_0_0, eps_start_0_, eps_end__0, eps_other, f1_any, precision_any, recall_any, f1_famous, precision_famous, recall_famous, f1_mean, precision_mean, recall_mean,\n",
    "grid_search_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        # \"eps_0_0\",\n",
    "        \"eps_start_0_\",\n",
    "        # \"eps_end__0\",\n",
    "        \"eps_other\",\n",
    "        \"f1_any\",\n",
    "        \"precision_any\",\n",
    "        \"recall_any\",\n",
    "        \"f1_famous\",\n",
    "        \"precision_famous\",\n",
    "        \"recall_famous\",\n",
    "        \"f1_mean\",\n",
    "        \"precision_mean\",\n",
    "        \"recall_mean\",\n",
    "    ]\n",
    ")\n",
    "# finally loop on all combiinations of epsilons in all dictionaries\n",
    "from itertools import product\n",
    "\n",
    "# Create a list of all combinations of epsilons\n",
    "eps_combinations = list(\n",
    "    product(\n",
    "        eps_grid[\"start_0_\"], eps_grid[\"other\"]\n",
    "    )\n",
    ") # eps_grid[\"0_0\"], eps_grid[\"end__0\"]\n",
    "best_f1_mean = -1\n",
    "best_precision_mean= -1\n",
    "best_config = None\n",
    "clusters_df[\"subcluster\"] = None\n",
    "clusters_df[\"cluster_1\"] = clusters_df[\"cluster_0\"]\n",
    "clusters_best_config_df = clusters_df.copy()\n",
    "# Loop through each combination of epsilons\n",
    "for eps_combination in tqdm(eps_combinations, desc=\"Processing epsilon combinations\"):\n",
    "    clusters_copy = clusters_df.copy()\n",
    "    clusters_copy = clusters_copy.set_index(\"full_name\")\n",
    "\n",
    "    eps_start_0_, eps_other = eps_combination # eps_0_0, eps_end__0\n",
    "    # Create a new row for the results DataFrame\n",
    "    new_row = {\n",
    "        # \"eps_0_0\": eps_0_0,\n",
    "        \"eps_start_0_\": eps_start_0_,\n",
    "        # \"eps_end__0\": eps_end__0,\n",
    "        \"eps_other\": eps_other,\n",
    "    }\n",
    "\n",
    "    # Update clusters_df with the new cluster_1 and subcluster for all dictionaries matching on 'full_name'\n",
    "    for cluster_type, dict_data, eps in zip(\n",
    "        [\"start_0_\", \"other\"], # \"0_0\", \"end__0\", \n",
    "        [dict_start_0_,dict_other], # dict_0_0,  dict_end__0, \n",
    "        [eps_start_0_, eps_other], # eps_0_0, eps_end__0, \n",
    "    ):\n",
    "        df = dict_data[eps].copy().set_index(\"full_name\")\n",
    "        clusters_copy.update(df[[\"subcluster\", \"cluster_1\"]])\n",
    "\n",
    "    # reset normal index\n",
    "    clusters_copy.reset_index(inplace=True)\n",
    "    \n",
    "    clusters_copy[\"cluster_1\"] = clusters_copy[\"cluster_1\"].apply(\n",
    "        lambda x: None if is_noise(x) else x\n",
    "    )\n",
    "    cluster_map = dict(zip(clusters_copy[\"full_name\"], clusters_copy[\"cluster_1\"]))\n",
    "\n",
    "    # Compute metrics on the updated clusters_df\n",
    "    # Apply predictions\n",
    "    eval_df_any = apply_predictions(\n",
    "        eval_df_any, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    "    )\n",
    "    metrics_any = compute_metrics(eval_df_any[\"final_label\"], eval_df_any[\"prediction\"])\n",
    "    eval_df_famous = apply_predictions(\n",
    "        eval_df_famous, cluster_map, col_el_1=\"full_name\", col_el_2=\"alias\"\n",
    "    )\n",
    "    metrics_famous = compute_metrics(\n",
    "        eval_df_famous[\"final_label\"], eval_df_famous[\"prediction\"]\n",
    "    )\n",
    "\n",
    "    # Add metrics to the new row\n",
    "    new_row.update(\n",
    "        {\n",
    "            \"f1_any\": metrics_any[\"f1\"],\n",
    "            \"precision_any\": metrics_any[\"precision\"],\n",
    "            \"recall_any\": metrics_any[\"recall\"],\n",
    "            \"f1_famous\": metrics_famous[\"f1\"],\n",
    "            \"precision_famous\": metrics_famous[\"precision\"],\n",
    "            \"recall_famous\": metrics_famous[\"recall\"],\n",
    "            \"f1_mean\": (metrics_any[\"f1\"] + metrics_famous[\"f1\"]) / 2,\n",
    "            \"precision_mean\": (metrics_any[\"precision\"] + metrics_famous[\"precision\"])\n",
    "            / 2,\n",
    "            \"recall_mean\": (metrics_any[\"recall\"] + metrics_famous[\"recall\"]) / 2,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check if this is the best configuration so far. if yes, then update best_config and best_f1_mean\n",
    "    if new_row[\"precision_mean\"] > best_precision_mean:\n",
    "        print(f\"--- New best precision_mean: {round(new_row['precision_mean'],2)} with eps: {eps_combination}\")\n",
    "        best_precision_mean = new_row[\"precision_mean\"]\n",
    "        \n",
    "    if new_row[\"f1_mean\"] > best_f1_mean:\n",
    "        print(f\"--- New best f1_mean: {round(new_row['f1_mean'],2)} with eps: {eps_combination}\")\n",
    "        clusters_best_config_df = clusters_copy.copy()\n",
    "        best_f1_mean = new_row[\"f1_mean\"]\n",
    "        best_config = {\n",
    "            # \"eps_0_0\": eps_0_0,\n",
    "            \"eps_start_0_\": eps_start_0_,\n",
    "            # \"eps_end__0\": eps_end__0,\n",
    "            \"eps_other\": eps_other,\n",
    "            \"f1_mean\": best_f1_mean,\n",
    "            \"precision_mean\": new_row[\"precision_mean\"],\n",
    "            \"recall_mean\": new_row[\"recall_mean\"],\n",
    "            \"f1_any\": new_row[\"f1_any\"],\n",
    "            \"precision_any\": new_row[\"precision_any\"],\n",
    "            \"recall_any\": new_row[\"recall_any\"],\n",
    "            \"f1_famous\": new_row[\"f1_famous\"],\n",
    "            \"precision_famous\": new_row[\"precision_famous\"],\n",
    "            \"recall_famous\": new_row[\"recall_famous\"],\n",
    "        }\n",
    "        # convert all values in best_config to float with 2 decimal places\n",
    "        best_config = {k: round(float(v), 2) for k, v in best_config.items()}\n",
    "\n",
    "    # Append the new row to the results DataFrame\n",
    "    grid_search_results = pd.concat(\n",
    "        [grid_search_results, pd.DataFrame([new_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Save grid search results to Excel\n",
    "grid_search_results.to_excel(results_xlsx, index=False)\n",
    "# print and save best configuration to JSON\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "with open(best_config_json, 'w') as f:\n",
    "    json.dump(best_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63dc114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel format the clusters_best_config_df\n",
    "clusters_best_config_df.to_excel(\n",
    "    output_dir / 'clusters_best_config.xlsx', index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb3c82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_1\n",
       "6_3_17      453\n",
       "6_3_2       118\n",
       "23_1_2       95\n",
       "8_1_1        74\n",
       "38_1_1       65\n",
       "356_1_1      62\n",
       "6_45_1       54\n",
       "6_7_2        49\n",
       "340_3_1      49\n",
       "6_10_1       42\n",
       "6_105_1      40\n",
       "6_74_2       39\n",
       "219_1_1      38\n",
       "23_5_1       38\n",
       "20_1_1       37\n",
       "14_8_2       35\n",
       "787_2_1      32\n",
       "9_1_1        31\n",
       "28_9_1       31\n",
       "6_4_1        30\n",
       "14_2_1       29\n",
       "235_1_1      29\n",
       "6_3_13       28\n",
       "260_3_1      28\n",
       "354_1_1      27\n",
       "9_3_1        27\n",
       "6_2_4        26\n",
       "6_4_2        26\n",
       "6_2_2        25\n",
       "242_1_2      25\n",
       "60_2_1       24\n",
       "75_1_2       23\n",
       "96_1_1       23\n",
       "6_25_2       23\n",
       "62_1_1       23\n",
       "6_17_1       23\n",
       "9_5_1        22\n",
       "6_191_1      22\n",
       "133_2_1      22\n",
       "75_1_1       21\n",
       "197_1_1      21\n",
       "173_4_1      21\n",
       "133_1_2      21\n",
       "1094_1_1     20\n",
       "441_2_2      20\n",
       "6_116_1      20\n",
       "248_5_3      20\n",
       "0_1_524      20\n",
       "28_7_1       20\n",
       "340_2_1      20\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show ranking off most popular clusters using cluster_1\n",
    "clusters_table_xlsx='data/W03/from_notebooks/R06_additional_grid_search/v5/clusters_best_config.xlsx'\n",
    "# clusters_table_xlsx = 'data/W02/R02_evaluate_model_performance/clusters_table.xlsx'\n",
    "clusters_df = pd.read_excel(clusters_table_xlsx)\n",
    "display(\n",
    "    clusters_df['cluster_1'].value_counts().head(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f4c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P05_refine_dedup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
