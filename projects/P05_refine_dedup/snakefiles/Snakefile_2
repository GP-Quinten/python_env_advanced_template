# This Snakefile is part of the P05_refine_dedup project.
rule all:
    input:
        # # R01
        # "data/W02/R01_train_model/grid_search_results.xlsx",
        # "data/W02/R01_train_model/best_config.json",
        # R01_raw
        "data/W02/R01_train_model_on_raw_registry_names/grid_search_results.xlsx",
        "data/W02/R01_train_model_on_raw_registry_names/best_config.json",
        # # R02
        # "data/W02/R02_evaluate_model_performance/clusters_table.xlsx",
        # "data/W02/R02_evaluate_model_performance/prediction_results_any_reg.xlsx",
        # "data/W02/R02_evaluate_model_performance/prediction_results_famous_reg.xlsx",
        # "data/W02/R02_evaluate_model_performance/performance_report_any_reg.json",
        # "data/W02/R02_evaluate_model_performance/performance_report_famous_reg.json",
        # R03

### ---------------------------------------------- ######
### --- Workflow 2 - Assess performance for method 1 with DBSCan  --- ###

# First, train a dbscan model to find the best clustering model (best parameters) based on the refined evaluation dataset, using grid search.
# First apply DBScan with primary, then on the noise of this primary clustering, and finally on the big clusters of this primary clustering.
rule R01_train_model:
    input:
        script="src/scripts/W02/S01_train_model.py",
        # evaluation_dataset="../../datasets/004_registry_names_deduplication_eval_dataset/official_reg_eval_dataset.xlsx",
        evaluation_dataset="data/W01/R03_eval_pairs_similarity_assessment_with_llm/final/famous_assessed_pairs.xlsx",
        # params_ranges_config_json="data/W02/R01_train_model/params_ranges_config.json"
    output:
        # in csv
        best_config_json="data/W02/R01_train_model/best_config.json",
        grid_search_results_xlsx="data/W02/R01_train_model/grid_search_results.xlsx",
    params:
        s3_input_embeddings = "registry_data_catalog_experiments/P05_refine_dedup/raw_registry_names_embeddings.parquet",
    log:
        "logs/W02/R01_train_model.log"
    shell:
        """
        PYTHONPATH=$(pwd) python -u {input.script} \\
            --evaluation_dataset {input.evaluation_dataset} \\
            --best_config_json {output.best_config_json} \\
            --grid_search_results_xlsx {output.grid_search_results_xlsx} \\
            --s3_input_embeddings {params.s3_input_embeddings} \\
            2>&1 | tee {log}
        """

rule R01_train_model_on_raw_registry_names:
    input:
        script="src/scripts/W02/S01_train_model_on_raw_registry_names.py",
        # evaluation_dataset="../../datasets/004_registry_names_deduplication_eval_dataset/official_reg_eval_dataset.xlsx",
        evaluation_dataset="data/W01/R03_eval_pairs_similarity_assessment_with_llm/final/famous_assessed_pairs.xlsx",
        # params_ranges_config_json="data/W02/R01_train_model/params_ranges_config.json"
    output:
        # in csv
        best_config_json="data/W02/R01_train_model_on_raw_registry_names/best_config.json",
        grid_search_results_xlsx="data/W02/R01_train_model_on_raw_registry_names/grid_search_results.xlsx",
    params:
        s3_input_embeddings = "registry_data_catalog_experiments/P05_refine_dedup/raw_registry_names_embeddings.parquet",
    log:
        "logs/W02/R01_train_model_on_raw_registry_names.log"
    shell:
        """
        PYTHONPATH=$(pwd) python -u {input.script} \\
            --evaluation_dataset {input.evaluation_dataset} \\
            --best_config_json {output.best_config_json} \\
            --grid_search_results_xlsx {output.grid_search_results_xlsx} \\
            --s3_input_embeddings {params.s3_input_embeddings} \\
            2>&1 | tee {log}
        """

# Then we apply this best clustering model to the training dataset and assess its performance on the evaluation dataset.
# classification task. precision, recall, f1-score, accuracy
rule R02_evaluate_model_performance:
    input:
        script="src/scripts/W02/S02_evaluate_model_performance.py",
        # evaluation_dataset_any="../../datasets/004_registry_names_deduplication_eval_dataset/eval_dataset_any_official_reg.xlsx",
        evaluation_dataset_any="data/W01/R03_eval_pairs_similarity_assessment_with_llm/final/any_assessed_pairs.xlsx",
        # evaluation_dataset_famous="../../datasets/004_registry_names_deduplication_eval_dataset/eval_dataset_famous_official_reg.xlsx",
        evaluation_dataset_famous="data/W01/R03_eval_pairs_similarity_assessment_with_llm/final/famous_assessed_pairs.xlsx",
        best_config="etc/best_config.json", # this is the best config obtained from the previous step, in json format
    output:
        clusters_table_xlsx="data/W02/R02_evaluate_model_performance/clusters_table.xlsx", # table of clusters with their size, number of pairs, number of aliases, etc.
        prediction_any_results_xlsx="data/W02/R02_evaluate_model_performance/prediction_results_any_reg.xlsx", # table of predictions for eval dataset with any pairs (simply adding a column with the predicted 1 or 0 for each pair)
        prediction_famous_results_xlsx="data/W02/R02_evaluate_model_performance/prediction_results_famous_reg.xlsx", # table of predictions for eval dataset with famous pairs (simply adding a column with the predicted 1 or 0 for each pair)
        performance_any_report_json="data/W02/R02_evaluate_model_performance/performance_report_any_reg.json",
        performance_famous_report_json="data/W02/R02_evaluate_model_performance/performance_report_famous_reg.json",
    params:
        s3_input_embeddings = "registry_data_catalog_experiments/P05_refine_dedup/registry_names_embeddings.parquet",
    log:
        "logs/W02/R02_evaluate_model_performance.log"
    shell:
        """
        PYTHONPATH=$(pwd) python -u {input.script} \\
            --evaluation_dataset_any {input.evaluation_dataset_any} \\
            --evaluation_dataset_famous {input.evaluation_dataset_famous} \\
            --best_config {input.best_config} \\
            --clusters_table_xlsx {output.clusters_table_xlsx} \\
            --prediction_any_results_xlsx {output.prediction_any_results_xlsx} \\
            --prediction_famous_results_xlsx {output.prediction_famous_results_xlsx} \\
            --performance_any_report_json {output.performance_any_report_json} \\
            --performance_famous_report_json {output.performance_famous_report_json} \\
            --s3_input_embeddings {params.s3_input_embeddings} \\
            2>&1 | tee {log}
        """


# --- TO FILL AND CORRECT --- #
# Finally, evaluate the performance on EMA registries
# Land all the ema registries in the map of embeddings, and predict if they land in a cluster or not. 
# COmpute 'ema_transformation_rate' as the ratio of the number of ema regisries that land in a cluster to the total number of ema registries.
# COmpute total number of cluster in which EMA registries land (in theory, there should not have 2 ema registries in the same cluster, but it can happen if the clustering is not perfect).
# among the ema registries that land in a cluster (called transfo_ema_reg), retrieve all its aliases (from names in the saame cluster) and compute the number of aliases per transfo_ema_reg.
rule R03_evaluate_model_performance_on_ema_registries:
    input:
        script="src/scripts/W02/S03_evaluate_model_performance_on_ema_registries.py",
        # clusters_table_xlsx="data/W02/R02_evaluate_model_performance/clusters_table.xlsx",
        clusters_table_xlsx="data/W03/from_notebooks/R08_loop_grid_searchs_start_0/v1/clusters_table.xlsx",
    output:
        ema_prediction_results_xlsx="data/W02/R03_evaluate_model_performance_on_ema_registries/new/ema_prediction_results.xlsx",
        ema_performance_report_json="data/W02/R03_evaluate_model_performance_on_ema_registries/new/ema_performance_report.json",
    params:
        s3_input_embeddings = "registry_data_catalog_experiments/P05_refine_dedup/_testregistry_names_embeddings.parquet",
        s3_ema_embeddings = 'registry_data_catalog_experiments/P05_refine_dedup/ema_registry_names_embeddings.parquet',
    log:
        "logs/W02/R03_evaluate_model_performance_on_ema_registries_NEW.log"
    shell:
        """
        PYTHONPATH=$(pwd) python -u {input.script} \\
            --clusters_table_xlsx {input.clusters_table_xlsx} \\
            --ema_prediction_results_xlsx {output.ema_prediction_results_xlsx} \\
            --ema_performance_report_json {output.ema_performance_report_json} \\
            --s3_input_embeddings {params.s3_input_embeddings} \\
            --s3_ema_embeddings {params.s3_ema_embeddings} \\
            2>&1 | tee {log}
        """

# --- TO FILL AND CORRECT --- #
# R04 is evaluating the quality of the aliases of EMA registries
# Compute the quality of the aliases of transformed ema registries. 
# Meaning that, for each transformed ema registry, give each pair to the llm for it to judge if they are indeed the same or not
rule R04_evaluate_aliases_of_transformed_ema_registries:
    input:
        script="src/scripts/W02/S04_evaluate_aliases_of_transformed_ema_registries.py",
        # ema_registries_embeddings="projects/P05_refine_dedup/data/W01/R04_embed_ema_registry_names/ema_registry_names_embeddings.parquet",
        ema_registries_embeddings="data/W01/R04_embed_ema_registry_names/ema_registry_names_embeddings.parquet",
        prompt_txt = "etc/prompts/refinement/prompt_compare_registry_names.txt",
        model_config = "etc/configs/{MODEL}_config.json",
    output:
        assessed_aliases_xlsx="data/W02/R04_evaluate_aliases_of_transformed_ema_registries/new/assessed_aliases.xlsx",
    params:
        s3_input_embeddings_dir = "registry_data_catalog_experiments/P05_refine_dedup/registry_name_embeddings",
    log:
        "logs/W02/R04_evaluate_aliases_of_transformed_ema_registries_NEW.log"
    shell:
        """
        PYTHONPATH=$(pwd) python -u {input.script} \\
            --ema_registries_embeddings {input.ema_registries_embeddings} \\
            --assessed_aliases_xlsx {output.assessed_aliases_xlsx} \\
            --prompt_txt {input.prompt_txt} \\
            --model_config {input.model_config} \\
            --s3_input_embeddings_dir {params.s3_input_embeddings_dir} \\
            2>&1 | tee {log}
        """