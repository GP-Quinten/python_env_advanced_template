{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92385c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = \"/home/gpinon/more_europa/clean_rdc_experiments/projects/P04_official_reg_db_creation\"\n",
    "os.chdir(working_dir)\n",
    "print(f\"Changed working directory to {working_dir}\")\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import weaviate\n",
    "\n",
    "from src.p04_official_reg_db_creation import config\n",
    "import llm_backends\n",
    "from llm_backends.cache import DiskCacheStorage\n",
    "from llm_backends.mistral import dummy_config\n",
    "from llm_backends.openai import dummy_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9991b",
   "metadata": {},
   "source": [
    "# 0. Raw publications loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bf01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track time of execution\n",
    "start_time = time.time()\n",
    "\n",
    "weaviate_client = weaviate.connect_to_custom(**config.WEAVIATE_PROD_CONF)\n",
    "collections = weaviate_client.collections  #\n",
    "# load publications\n",
    "collection_publications = collections.get(\"Publication_v2\")\n",
    "# load data source names\n",
    "items = []\n",
    "for item in collection_publications.iterator(include_vector=False):\n",
    "    # Extract subset of properties\n",
    "    items.append(\n",
    "        {\n",
    "            k: v\n",
    "            for k, v in item.properties.items()\n",
    "            if k\n",
    "            in [\n",
    "                \"object_id\",\n",
    "                \"title\",\n",
    "                \"abstract\",\n",
    "                \"data_source_name\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "# close weaviate connection\n",
    "weaviate_client.close()\n",
    "# df_items = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "batch_size = 2000\n",
    "shuffled_items = items.copy()\n",
    "random.shuffle(shuffled_items)\n",
    "\n",
    "# ensure output directory exists\n",
    "output_dir = Path(\"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/0_raw_publications\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "batch_number = 1\n",
    "for i in range(0, len(shuffled_items), batch_size):\n",
    "    batch_items = shuffled_items[i : i + batch_size]\n",
    "    output_file = output_dir / f\"{batch_number}.jsonl\"\n",
    "    with output_file.open(\"w\") as f:\n",
    "        for item in batch_items:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    batch_number += 1\n",
    "\n",
    "print(f\"Loading time: {(time.time() - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1baf61e",
   "metadata": {},
   "source": [
    "# 1. First filter publications that are not registry related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD = \"registry_related\"\n",
    "MODEL = \"small_mistral\"\n",
    "\n",
    "# INPUTS\n",
    "raw_publications_dir = \"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/0_raw_publications\"  # Directory containing batch files\n",
    "prompt_txt = f\"etc/prompts/extraction/prompt_{FIELD}.txt\"\n",
    "model_config = f\"etc/configs/{MODEL}_config.json\"\n",
    "\n",
    "# OUTPUTS - template paths that will be formatted with batch number\n",
    "output_registry_related_template = \"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/01_{FIELD}_publications_{batch}.json\"\n",
    "output_all_records_template = \"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/01_{FIELD}_raw_inferences_{batch}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration\n",
    "with open(model_config, \"r\", encoding=\"utf-8\") as f:\n",
    "    model_cfg = json.load(f)\n",
    "\n",
    "model_name = model_cfg.get(\"model\", \"unknown\")\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "# Load the annotation prompt\n",
    "with open(prompt_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "    annotation_prompt = f.read().strip()\n",
    "\n",
    "# Get list of all batch files\n",
    "batch_files = sorted(\n",
    "    Path(raw_publications_dir).glob(\"*.jsonl\"), key=lambda p: int(p.stem)\n",
    ")\n",
    "# limit to 1 batch for testing\n",
    "batch_files = batch_files[:1]  # Uncomment to limit to 1 batch for testing\n",
    "print(f\"Found {len(batch_files)} batch files to process\")\n",
    "\n",
    "# Create the base output directory\n",
    "out_dir = Path(output_registry_related_template.format(FIELD=FIELD, batch=1)).parent\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Process each batch file separately\n",
    "total_registry_related = 0\n",
    "total_not_registry_related = 0\n",
    "total_records = 0\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    batch_num = batch_file.stem  # Get batch number from filename\n",
    "    print(f\"\\nProcessing batch {batch_num}...\")\n",
    "\n",
    "    # Format output paths for this batch\n",
    "    output_registry_related_records = output_registry_related_template.format(\n",
    "        FIELD=FIELD, batch=batch_num\n",
    "    )\n",
    "    output_all_records_jsonl = output_all_records_template.format(\n",
    "        FIELD=FIELD, batch=batch_num\n",
    "    )\n",
    "\n",
    "    # Load records from this batch\n",
    "    records = []\n",
    "    with open(batch_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            object_id = record.get(\"object_id\", \"<unknown>\")\n",
    "            records.append(record)\n",
    "    print(f\"Loaded {len(records)} records from batch {batch_num}\")\n",
    "\n",
    "    # Prepare prompts for LLMs\n",
    "    prompts_items = []\n",
    "    records = records[:5] # Uncomment to limit to 5 records for testing\n",
    "    for rec in records:\n",
    "        object_id = rec.get(\"object_id\", \"<unknown>\")\n",
    "        title = rec.get(\"title\", \"<no title>\")\n",
    "        abstract = rec.get(\"abstract\", \"<no abstract>\")\n",
    "        full_prompt = f\"{annotation_prompt}\\nText_to_analyze:\\nTitle: {title}\\nAbstract: {abstract}\"\n",
    "        prompts_items.append({\"prompt\": full_prompt, \"custom_id\": object_id})\n",
    "\n",
    "    # Create a list to store the records with object_id, prompt, and raw response\n",
    "    prompt_response_records = []\n",
    "\n",
    "    # Run batch inference\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting batch inference for batch {batch_num} with {model_name}...\")\n",
    "    llm_responses = []\n",
    "\n",
    "    is_openai_model = \"openai\" in model_config.lower()\n",
    "    is_mistral_model = \"istral\" in model_config.lower()\n",
    "    if is_mistral_model:\n",
    "        backend = llm_backends.MistralBatchBackend(\n",
    "            api_key=os.getenv(\"MISTRAL_API_KEY\"), cache_storage=DiskCacheStorage()\n",
    "        )\n",
    "    elif is_openai_model:\n",
    "        backend = llm_backends.OpenAIAsyncBackend(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"), cache_storage=DiskCacheStorage()\n",
    "        )\n",
    "\n",
    "    raw_responses = backend.infer_many(\n",
    "        prompt_items=prompts_items,\n",
    "        model_config=model_cfg,\n",
    "    )\n",
    "\n",
    "    for raw_response in raw_responses:\n",
    "        # Store the raw response with object_id and prompt for the records file\n",
    "        prompt_obj = next(\n",
    "            (p for p in prompts_items if p[\"custom_id\"] == raw_response[\"custom_id\"]),\n",
    "            None,\n",
    "        )\n",
    "        if prompt_obj:\n",
    "            prompt_response_records.append(\n",
    "                {\n",
    "                    \"object_id\": raw_response[\"custom_id\"],\n",
    "                    \"prompt\": prompt_obj[\"prompt\"],\n",
    "                    \"llm_response\": raw_response,\n",
    "                }\n",
    "            )\n",
    "            # parse raw response\n",
    "            parsed_response = backend._parse_response(raw_response)\n",
    "            parsed_response[\"custom_id\"] = raw_response.get(\"custom_id\", \"\")\n",
    "            llm_responses.append(parsed_response)\n",
    "\n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(\n",
    "        f\"Batch {batch_num} inference completed with {len(llm_responses)} responses in {elapsed_total:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Build results for this batch\n",
    "    results = []\n",
    "    for rec in records:\n",
    "        object_id = rec.get(\"object_id\", \"<unknown>\")\n",
    "        title = rec.get(\"title\", \"<no title>\")\n",
    "        abstract = rec.get(\"abstract\", \"<no abstract>\")\n",
    "\n",
    "        resp = next((x for x in llm_responses if x[\"custom_id\"] == object_id), None)\n",
    "\n",
    "        if resp:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"object_id\": object_id,\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"llm_response\": resp,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Count statistics for this batch\n",
    "    registry_related_records = [\n",
    "        rec for rec in results if rec[\"llm_response\"].get(\"registry_related\") == \"yes\"\n",
    "    ]\n",
    "    not_registry_related_records = [\n",
    "        rec for rec in results if rec[\"llm_response\"].get(\"registry_related\") == \"no\"\n",
    "    ]\n",
    "    print(\n",
    "        f\"Batch {batch_num}: Found {len(registry_related_records)} records related to registry\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Batch {batch_num}: Found {len(not_registry_related_records)} records not related to registry\"\n",
    "    )\n",
    "\n",
    "    batch_total = len(results)\n",
    "    rel_percentage = (\n",
    "        len(registry_related_records) / batch_total * 100 if batch_total > 0 else 0\n",
    "    )\n",
    "    print(f\"Batch {batch_num}: {rel_percentage:.2f}% of records are registry-related\")\n",
    "\n",
    "    # Update totals\n",
    "    total_registry_related += len(registry_related_records)\n",
    "    total_not_registry_related += len(not_registry_related_records)\n",
    "    total_records += batch_total\n",
    "\n",
    "    # Save the batch results\n",
    "    with open(output_registry_related_records, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(registry_related_records, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    with open(output_all_records_jsonl, \"w\", encoding=\"utf-8\") as fp:\n",
    "        for record in prompt_response_records:\n",
    "            fp.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved batch {batch_num} results to {output_registry_related_records}\")\n",
    "    print(f\"Saved batch {batch_num} raw inferences to {output_all_records_jsonl}\")\n",
    "\n",
    "# Print overall statistics\n",
    "print(\"\\n--- Overall Statistics ---\")\n",
    "print(f\"Total records processed: {total_records}\")\n",
    "print(f\"Total registry-related records: {total_registry_related}\")\n",
    "print(f\"Total not registry-related records: {total_not_registry_related}\")\n",
    "if total_records > 0:\n",
    "    print(\n",
    "        f\"Overall percentage of registry-related records: {total_registry_related / total_records * 100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86548e08",
   "metadata": {},
   "source": [
    "# 2. Registry Name extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96eae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD = \"registry_name\"\n",
    "MODEL = \"small_mistral\"\n",
    "\n",
    "# INPUTS\n",
    "registry_related_publications = \"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/01_registry_related_publications.json\"\n",
    "prompt_txt = prompt_txt=f\"etc/prompts/extraction/prompt_{FIELD}.txt\"\n",
    "model_config=f\"etc/configs/{MODEL}_config.json\"\n",
    "\n",
    "# OUTPUTS\n",
    "output_json = f\"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/02_{FIELD}_extractions.json\"\n",
    "output_records_jsonl = f\"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/02_{FIELD}_raw_inferences.jsonl\"\n",
    "output_registry_names_list = f\"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/02_{FIELD}_list.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = Path(output_json).parent\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure output records directory exists\n",
    "records_dir = Path(output_records_jsonl).parent\n",
    "records_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load model configuration\n",
    "with open(model_config, \"r\", encoding=\"utf-8\") as f:\n",
    "    model_cfg = json.load(f)\n",
    "\n",
    "model_name = model_cfg.get(\"model\", \"unknown\")\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "# Load the annotation prompt\n",
    "with open(prompt_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "    annotation_prompt = f.read().strip()\n",
    "\n",
    "# Load and filter PubMed records\n",
    "records = []\n",
    "with open(registry_related_publications, \"r\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)\n",
    "        object_id = record.get(\"object_id\", \"<unknown>\")\n",
    "        records.append(record)\n",
    "        \n",
    "print(f\"Loaded {len(records)} records for registry_related filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393078bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prompts for LLMs\n",
    "prompts_items = []\n",
    "# records = records[:5] # Limit to first 5 records for testing\n",
    "for rec in records:\n",
    "    object_id = rec.get(\"object_id\", \"<unknown>\")\n",
    "    title = rec.get(\"title\", \"<no title>\")\n",
    "    abstract = rec.get(\"abstract\", \"<no abstract>\")\n",
    "    full_prompt = f\"{annotation_prompt}\\nText_to_analyze:\\nTitle: {title}\\nAbstract: {abstract}\"\n",
    "    prompts_items.append({\"prompt\": full_prompt, \"custom_id\": object_id})\n",
    "\n",
    "# Create a list to store the records with object_id, prompt, and raw response\n",
    "prompt_response_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f55384",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Run batch inference based on model type\n",
    "print(f\"Starting batch inference with {model_name}...\")\n",
    "llm_responses = []\n",
    "\n",
    "is_openai_model = \"openai\" in model_config.lower()\n",
    "# if \"istral\" in the name of llm_judge_model_config, then we are using Mistral model\n",
    "is_mistral_model = \"istral\" in model_config.lower()\n",
    "if is_mistral_model:\n",
    "    backend = llm_backends.MistralBatchBackend(\n",
    "        api_key=os.getenv(\"MISTRAL_API_KEY\"), cache_storage=DiskCacheStorage()\n",
    "    )\n",
    "elif is_openai_model:\n",
    "    backend = llm_backends.OpenAIAsyncBackend(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"), cache_storage=DiskCacheStorage()\n",
    "    )\n",
    "\n",
    "raw_responses = backend.infer_many(\n",
    "    prompt_items=prompts_items,\n",
    "    model_config=model_cfg,\n",
    ")\n",
    "\n",
    "for raw_response in raw_responses:\n",
    "    # Store the raw response with object_id and prompt for the records file\n",
    "    prompt_obj = next(\n",
    "        (p for p in prompts_items if p[\"custom_id\"] == raw_response[\"custom_id\"]), None\n",
    "    )\n",
    "    if prompt_obj:\n",
    "        prompt_response_records.append(\n",
    "            {\n",
    "                \"object_id\": raw_response[\"custom_id\"],\n",
    "                \"prompt\": prompt_obj[\"prompt\"],\n",
    "                \"llm_response\": raw_response,\n",
    "            }\n",
    "        )\n",
    "        # parse raw response\n",
    "        parsed_response = backend._parse_response(raw_response)\n",
    "        parsed_response[\"custom_id\"] = raw_response.get(\"custom_id\", \"\")\n",
    "        # print(response)\n",
    "        llm_responses.append(parsed_response)\n",
    "\n",
    "print(f\"Batch inference completed with {len(llm_responses)} responses\")\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"Total time for batch inference: {elapsed_total:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a18906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build results DataFrame\n",
    "results = []\n",
    "for rec in records:\n",
    "    object_id = rec.get(\"object_id\", \"<unknown>\")\n",
    "    title = rec.get(\"title\", \"<no title>\")\n",
    "    abstract = rec.get(\"abstract\", \"<no abstract>\")\n",
    "\n",
    "    resp = next((x for x in llm_responses if x[\"custom_id\"] == object_id), None)\n",
    "\n",
    "    if resp:\n",
    "        results.append(\n",
    "            {\n",
    "                \"object_id\": object_id,\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"llm_response\": resp,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved metadata extractions to {output_json}\")\n",
    "\n",
    "# Save records to JSONL file\n",
    "with open(output_records_jsonl, \"w\", encoding=\"utf-8\") as fp:\n",
    "    for record in prompt_response_records:\n",
    "        fp.write(json.dumps(record) + \"\\n\")\n",
    "print(f\"Saved {len(prompt_response_records)} records to {output_records_jsonl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c341f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the results from the output_json\n",
    "with open(output_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    publications_w_list_of_registries = json.load(f)\n",
    "\n",
    "# save the registry names instead of the publications\n",
    "registry_name_list = []\n",
    "# loop on the publications\n",
    "for publication in publications_w_list_of_registries:\n",
    "    # get the object_id and llm_response[\"List of Registry names\"]\n",
    "    object_id = publication[\"object_id\"]\n",
    "    list_of_registries = publication[\"llm_response\"].get(\"List of Registry names\", [])\n",
    "    index = 0\n",
    "    for registry in list_of_registries:\n",
    "        registry_name_list.append(\n",
    "            {\n",
    "                'index': index,\n",
    "                \"registry_name\": registry.get('registry_name', ''),\n",
    "                \"acronym\": registry.get('acronym', ''),\n",
    "                \"is_official\": registry.get('is_official', ''),\n",
    "                \"object_id\": object_id,\n",
    "            }\n",
    "        )\n",
    "        index += 1\n",
    "\n",
    "# print the number of registry names found using final index value\n",
    "print(f\"Found {index} registry names in total\")\n",
    "\n",
    "# Save the registry names list to a JSON file\n",
    "with open(output_registry_names_list, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(registry_name_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Inputs and Outputs for Registry Name Extractions ===\n",
    "\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# INPUTS:\n",
    "# Pattern to load all registry-related publication files (from Section 1 outputs)\n",
    "registry_publications_pattern = \"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/01_registry_related_publications_*.json\"\n",
    "print(\"Using registry publication files matching:\", registry_publications_pattern)\n",
    "\n",
    "# Batching parameter for the new aggregation\n",
    "batch_size_registry = 2000\n",
    "\n",
    "# Model and prompt specifications:\n",
    "FIELD = \"registry_name\"\n",
    "MODEL = \"small_mistral\"\n",
    "prompt_txt = f\"etc/prompts/extraction/prompt_{FIELD}.txt\"\n",
    "model_config = f\"etc/configs/{MODEL}_config.json\"\n",
    "\n",
    "# OUTPUTS:\n",
    "# Directories for saving the new batches and corresponding outputs\n",
    "raw_inferences_dir = Path(f\"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/02_{FIELD}_raw_inferences\")\n",
    "parsed_extractions_dir = Path(f\"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/02_{FIELD}_extractions\")\n",
    "registry_list_dir = Path(f\"data/from_scripts/SW01/nbtk_testing/R04_extraction_all/02_{FIELD}_list\")\n",
    "\n",
    "# Create the output directories if they do not exist\n",
    "raw_inferences_dir.mkdir(parents=True, exist_ok=True)\n",
    "parsed_extractions_dir.mkdir(parents=True, exist_ok=True)\n",
    "registry_list_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Output directories:\")\n",
    "print(\" Raw extractions:\", raw_inferences_dir)\n",
    "print(\" Parsed extractions:\", parsed_extractions_dir)\n",
    "print(\" Registry names list:\", registry_list_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc048574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "registry_files = sorted(glob.glob(registry_publications_pattern))\n",
    "print(f\"Found {len(registry_files)} registry-related publication files.\")\n",
    "\n",
    "all_registry_publications = []\n",
    "for file in registry_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        publications = json.load(f)\n",
    "        all_registry_publications.extend(publications)\n",
    "print(f\"Aggregated {len(all_registry_publications)} registry-related publications.\")\n",
    "\n",
    "# Load model configuration and annotation prompt\n",
    "with open(model_config, \"r\", encoding=\"utf-8\") as f:\n",
    "    model_cfg = json.load(f)\n",
    "model_name = model_cfg.get(\"model\", \"unknown\")\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "with open(prompt_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "    annotation_prompt = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(all_registry_publications)\n",
    "batches = [all_registry_publications[i : i + batch_size_registry] \n",
    "           for i in range(0, len(all_registry_publications), batch_size_registry)]\n",
    "print(f\"Divided into {len(batches)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf942ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each new batch, run the registry name extraction inference and save outputs.\n",
    "for idx, batch in enumerate(batches, start=1):\n",
    "    print(f\"\\nProcessing extraction batch {idx} with {len(batch)} records...\")\n",
    "    \n",
    "    # Prepare prompt items for inference\n",
    "    prompts_items = []\n",
    "    for rec in batch:\n",
    "        object_id = rec.get(\"object_id\", \"<unknown>\")\n",
    "        title = rec.get(\"title\", \"<no title>\")\n",
    "        abstract = rec.get(\"abstract\", \"<no abstract>\")\n",
    "        full_prompt = f\"{annotation_prompt}\\nText_to_analyze:\\nTitle: {title}\\nAbstract: {abstract}\"\n",
    "        prompts_items.append({\"prompt\": full_prompt, \"custom_id\": object_id})\n",
    "    \n",
    "    # Lists for storing raw and parsed responses\n",
    "    prompt_response_records = []\n",
    "    llm_responses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"Starting batch {idx} registry inference with {model_name}...\")\n",
    "\n",
    "    # Instantiate the backend based on the model configuration  \n",
    "    is_openai_model = \"openai\" in model_config.lower()\n",
    "    is_mistral_model = \"istral\" in model_config.lower()\n",
    "    if is_mistral_model:\n",
    "        backend = llm_backends.MistralBatchBackend(\n",
    "            api_key=os.getenv(\"MISTRAL_API_KEY\"), cache_storage=DiskCacheStorage()\n",
    "        )\n",
    "    elif is_openai_model:\n",
    "        backend = llm_backends.OpenAIAsyncBackend(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"), cache_storage=DiskCacheStorage()\n",
    "        )\n",
    "    \n",
    "    # Run inference\n",
    "    raw_responses = backend.infer_many(\n",
    "        prompt_items=prompts_items,\n",
    "        model_config=model_cfg,\n",
    "    )\n",
    "    \n",
    "    # Parse and record responses\n",
    "    for raw_response in raw_responses:\n",
    "        prompt_obj = next((p for p in prompts_items if p[\"custom_id\"] == raw_response[\"custom_id\"]), None)\n",
    "        if prompt_obj:\n",
    "            prompt_response_records.append({\n",
    "                \"object_id\": raw_response[\"custom_id\"],\n",
    "                \"prompt\": prompt_obj[\"prompt\"],\n",
    "                \"llm_response\": raw_response,\n",
    "            })\n",
    "            parsed_response = backend._parse_response(raw_response)\n",
    "            parsed_response[\"custom_id\"] = raw_response.get(\"custom_id\", \"\")\n",
    "            llm_responses.append(parsed_response)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Batch {idx} completed with {len(llm_responses)} responses in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # Build parsed results by matching each publication with its inference response\n",
    "    results = []\n",
    "    for rec in batch:\n",
    "        object_id = rec.get(\"object_id\", \"<unknown>\")\n",
    "        title = rec.get(\"title\", \"<no title>\")\n",
    "        abstract = rec.get(\"abstract\", \"<no abstract>\")\n",
    "        resp = next((x for x in llm_responses if x[\"custom_id\"] == object_id), None)\n",
    "        if resp:\n",
    "            results.append({\n",
    "                \"object_id\": object_id,\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"llm_response\": resp,\n",
    "            })\n",
    "    \n",
    "    # Extract registry names list from parsed responses\n",
    "    registry_name_list = []\n",
    "    for publication in results:\n",
    "        object_id = publication[\"object_id\"]\n",
    "        list_of_registries = publication[\"llm_response\"].get(\"List of Registry names\", [])\n",
    "        for i, registry in enumerate(list_of_registries):\n",
    "            registry_name_list.append({\n",
    "                \"index\": i,\n",
    "                \"registry_name\": registry.get(\"registry_name\", \"\"),\n",
    "                \"acronym\": registry.get(\"acronym\", \"\"),\n",
    "                \"is_official\": registry.get(\"is_official\", \"\"),\n",
    "                \"object_id\": object_id,\n",
    "            })\n",
    "    \n",
    "    # Define output file paths for the current batch using the naming convention\n",
    "    raw_out_file = raw_inferences_dir / f\"{idx}.jsonl\"\n",
    "    parsed_out_file = parsed_extractions_dir / f\"{idx}.json\"\n",
    "    list_out_file = registry_list_dir / f\"{idx}.json\"\n",
    "    \n",
    "    # Save raw extractions (each record on a new line in a JSONL file)\n",
    "    with open(raw_out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in prompt_response_records:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\"Saved raw extractions to {raw_out_file}\")\n",
    "    \n",
    "    # Save parsed extractions (a JSON file with an array of records)\n",
    "    with open(parsed_out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Saved parsed extractions to {parsed_out_file}\")\n",
    "    \n",
    "    # Save extracted registry names list (JSON file)\n",
    "    with open(list_out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(registry_name_list, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Saved registry names list to {list_out_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P04_official_reg_db_creation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
