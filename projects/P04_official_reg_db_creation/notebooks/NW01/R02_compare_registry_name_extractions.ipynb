{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a890f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = \"/home/gpinon/more_europa/clean_rdc_experiments/projects/P04_official_reg_db_creation\"\n",
    "os.chdir(working_dir)\n",
    "print(f\"Changed working directory to {working_dir}\")\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.p04_official_reg_db_creation import config\n",
    "import llm_backends\n",
    "from llm_backends.cache import DiskCacheStorage\n",
    "from llm_backends.mistral import dummy_config\n",
    "from llm_backends.openai import dummy_config\n",
    "from p04_official_reg_db_creation.config import MAPPING, INVERSE_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fadf054",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TYPE =\"eval\" # \"test\" # \n",
    "RAW_PUBLICATIONS_DICT = {\n",
    "    \"eval\": \"../../datasets/001_publications_dataset/publications_dataset.jsonl\",\n",
    "    \"test\": \"../../datasets/001_publications_dataset/prod_publication_test_dataset.jsonl\",\n",
    "}\n",
    "FIELD = \"registry_name\"\n",
    "MODEL = \"small_mistral\"\n",
    "\n",
    "# Load environment variables from .env file and get API key\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_extractions=f\"data/from_notebooks/NW01/R01_extraction/{MODEL}/{FIELD}/{FIELD}_extractions.json\"\n",
    "eval_dataset_json=f\"../../datasets/005_evaluate_extraction_process_datasets/{FIELD}/final_eval_dataset.json\"\n",
    "llm_judge_model_config= \"etc/configs/gpt4o_openai_config.json\" # \"etc/configs/large_mistral_config.json\" # \"etc/configs/gpt4_1_openai_config.json\" # \n",
    "prompt_llm_judge = f\"etc/prompts/llm_as_a_judge/compare_{FIELD}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json=f\"data/from_notebooks/NW01/R02_comparison/{MODEL}/{FIELD}/compare_{FIELD}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb351805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = Path(output_json).parent\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get the mapped field name if it exists\n",
    "field_name = INVERSE_MAPPING.get(FIELD, FIELD)\n",
    "print(f\"Processing field: {FIELD} (mapped to {field_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prompt_llm_judge, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_template = f.read().strip()\n",
    "\n",
    "# Load the model configuration\n",
    "# if \"openai\" in the name of llm_judge_model_config, then we are using OpenAI model\n",
    "is_openai_model = \"openai\" in llm_judge_model_config.lower()\n",
    "# if \"istral\" in the name of llm_judge_model_config, then we are using Mistral model\n",
    "is_mistral_model = \"istral\" in llm_judge_model_config.lower()\n",
    "\n",
    "with open(llm_judge_model_config, \"r\", encoding=\"utf-8\") as f:\n",
    "    judge_model_cfg = json.load(f)\n",
    "print(f\"Using model config: {judge_model_cfg.get('model', 'unknown')}\")\n",
    "\n",
    "# Load metadata extractions\n",
    "with open(metadata_extractions, \"r\", encoding=\"utf-8\") as f:\n",
    "    extraction_records = json.load(f)\n",
    "print(f\"Loaded {len(extraction_records)} records from metadata extractions\")\n",
    "\n",
    "# Load evaluation dataset\n",
    "with open(eval_dataset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_records = json.load(f)\n",
    "print(f\"Loaded {len(eval_records)} records from evaluation dataset\")\n",
    "\n",
    "# Filter out records that need manual annotation\n",
    "filtered_eval_records = [\n",
    "    r for r in eval_records if not r.get(\"needs_manual_annotation\", False)\n",
    "]\n",
    "print(f\"Filtered to {len(filtered_eval_records)} records with completed annotations\")\n",
    "\n",
    "# Create a lookup dictionary for evaluation records by object_id\n",
    "eval_lookup = {r[\"object_id\"]: r for r in filtered_eval_records}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare records for processing\n",
    "processed_records = []\n",
    "llm_judge_prompts = []\n",
    "\n",
    "# Statistics counters\n",
    "stats = {\n",
    "    \"total_extracted\": len(extraction_records),\n",
    "    \"in_eval_dataset\": 0,\n",
    "    \"model_agreement\": 0,\n",
    "    \"one_model_unspecified\": 0,\n",
    "    \"need_llm_judge\": 0,\n",
    "    \"llm_same\": 0,\n",
    "    \"llm_different\": 0,\n",
    "}\n",
    "\n",
    "correct_field_col = f\"correct_{FIELD}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each extraction record\n",
    "for rec in extraction_records:\n",
    "    object_id = rec.get(\"object_id\")\n",
    "\n",
    "    # Check if this record exists in the evaluation dataset\n",
    "    if object_id not in eval_lookup:\n",
    "        continue\n",
    "\n",
    "    stats[\"in_eval_dataset\"] += 1\n",
    "    eval_rec = eval_lookup[object_id]\n",
    "\n",
    "    # Get the model's extracted value for this field\n",
    "    model_response = rec.get(\"llm_response\", {})\n",
    "\n",
    "    inferred_list = model_response.get(\"List of Registry names\", \"Not found\")\n",
    "    # print(f\"Processing record {object_id} with inferred_list: {inferred_list}\")\n",
    "    # inferred_value is a string of all the registry_name + (acronym) separated by commas\n",
    "    if isinstance(inferred_list, list):\n",
    "        inferred_value = \"\"\n",
    "        for registry in inferred_list:\n",
    "            # if acronym is not emrty string \"\"\n",
    "            # print(f\"Processing registry: {registry}\")\n",
    "            if registry[\"acronym\"]!= \"\":\n",
    "                inferred_value += f\"{registry['registry_name']} ({registry['acronym']}), \"\n",
    "            else:\n",
    "                inferred_value += f\"{registry['registry_name']}, \"\n",
    "        # Remove the trailing comma and space\n",
    "        inferred_value = inferred_value.rstrip(\", \")\n",
    "            \n",
    "    correct_value = eval_rec.get(correct_field_col, \"Not found\")\n",
    "    # print both values to compare\n",
    "    # print(f\"Object ID: {object_id}, Inferred: {inferred_value}, Correct: {correct_value}\")\n",
    "    # Create the basic record structure\n",
    "    output_record = {\n",
    "        \"object_id\": object_id,\n",
    "        \"title\": rec.get(\"title\", \"\"),\n",
    "        \"abstract\": rec.get(\"abstract\", \"\"),\n",
    "        f\"inferred_{FIELD}\": inferred_value,\n",
    "        correct_field_col: correct_value,\n",
    "    }\n",
    "\n",
    "    # Check for exact match (case insensitive)\n",
    "    if inferred_value.lower() == correct_value.lower():\n",
    "        output_record[\"final_label\"] = 1\n",
    "        output_record[\"labeling_reason\"] = \"model_agreement\"\n",
    "        stats[\"model_agreement\"] += 1\n",
    "        processed_records.append(output_record)\n",
    "        continue\n",
    "\n",
    "    # Check if one is \"Not specified\"\n",
    "    inferred_unspecified = (\n",
    "        inferred_value.lower() == \"not found\"\n",
    "        or inferred_value.lower() == \"not specified\"\n",
    "    )\n",
    "    correct_unspecified = (\n",
    "        correct_value.lower() == \"not found\"\n",
    "        or correct_value.lower() == \"not specified\"\n",
    "    )\n",
    "\n",
    "    if (inferred_unspecified and not correct_unspecified) or (\n",
    "        not inferred_unspecified and correct_unspecified\n",
    "    ):\n",
    "        output_record[\"final_label\"] = 0\n",
    "        output_record[\"labeling_reason\"] = \"one_model_unspecified\"\n",
    "        stats[\"one_model_unspecified\"] += 1\n",
    "        processed_records.append(output_record)\n",
    "        continue\n",
    "\n",
    "    # Need LLM judge\n",
    "    stats[\"need_llm_judge\"] += 1\n",
    "\n",
    "    # Prepare prompt for LLM judge\n",
    "    full_prompt = prompt_template.replace(\"{{content_a}}\", inferred_value)\n",
    "    full_prompt = full_prompt.replace(\"{{content_b}}\", correct_value)\n",
    "    # print the last 100 characters of the prompt\n",
    "    print('----')\n",
    "    # print all characters of the prompt after '</example6>'\n",
    "    stop = full_prompt.find('</example6>') + 13\n",
    "    print(full_prompt[stop:])\n",
    "    llm_judge_prompts.append({\"prompt\": full_prompt, \"custom_id\": object_id})\n",
    "\n",
    "    # Save the record for later updating with LLM judgment\n",
    "    processed_records.append(output_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# print running LLM as a judge with model\n",
    "print(f\"Running LLM as a judge with model: {judge_model_cfg.get('model', 'unknown')}\")\n",
    "# Run LLM judgments if needed\n",
    "if llm_judge_prompts:\n",
    "    print(f\"Running LLM judgment for {len(llm_judge_prompts)} records\")\n",
    "    if is_openai_model:\n",
    "        # Initialize the OpenAI backend\n",
    "        backend = llm_backends.OpenAIAsyncBackend(\n",
    "            api_key=OPENAI_API_KEY, cache_storage=DiskCacheStorage()\n",
    "        )\n",
    "    elif is_mistral_model:\n",
    "        # Initialize the Mistral backend\n",
    "        backend = llm_backends.MistralBatchBackend(\n",
    "            api_key=MISTRAL_API_KEY, cache_storage=DiskCacheStorage()\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Please use OpenAI or Mistral models.\")\n",
    "\n",
    "    # Perform batch inference\n",
    "    judge_results = backend.infer_many(llm_judge_prompts, judge_model_cfg)\n",
    "\n",
    "    # Process LLM judge results\n",
    "    for result in judge_results:\n",
    "        parsed_response = backend._parse_response(result)\n",
    "        parsed_response[\"custom_id\"] = result.get(\"custom_id\", \"\")\n",
    "        object_id = result[\"custom_id\"]\n",
    "\n",
    "        # Find the corresponding record\n",
    "        for record in processed_records:\n",
    "            if record[\"object_id\"] == object_id and \"final_label\" not in record:\n",
    "                if parsed_response[\"final_decision\"].lower() == \"same\":\n",
    "                    record[\"final_label\"] = 1\n",
    "                    record[\"labeling_reason\"] = \"llm_same\"\n",
    "                    stats[\"llm_same\"] += 1\n",
    "                else:\n",
    "                    record[\"final_label\"] = 0\n",
    "                    record[\"labeling_reason\"] = \"llm_different\"\n",
    "                    stats[\"llm_different\"] += 1\n",
    "\n",
    "                # Store LLM explanation\n",
    "                record[\"llm_explanation\"] = parsed_response[\"explanation\"]\n",
    "                break\n",
    "\n",
    "    # Ensure all records have the necessary fields\n",
    "    for record in processed_records:\n",
    "        if \"final_label\" not in record:\n",
    "            record[\"final_label\"] = (\n",
    "                0  # Default to not matching if we couldn't determine\n",
    "            )\n",
    "            record[\"labeling_reason\"] = \"undetermined\"\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total processing time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log statistics\n",
    "print(\"Comparison statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Count the final labels\n",
    "positive_labels = sum(1 for r in processed_records if r.get(\"final_label\") == 1)\n",
    "negative_labels = sum(1 for r in processed_records if r.get(\"final_label\") == 0)\n",
    "print(\n",
    "    f\"Final labels: {positive_labels} positive, {negative_labels} negative\"\n",
    ")\n",
    "\n",
    "# Detailed breakdown of LLM judge results\n",
    "if stats[\"need_llm_judge\"] > 0:\n",
    "    llm_same_percent = (stats[\"llm_same\"] / stats[\"need_llm_judge\"]) * 100\n",
    "    llm_different_percent = (stats[\"llm_different\"] / stats[\"need_llm_judge\"]) * 100\n",
    "    print(\n",
    "        f\"LLM judge breakdown: {stats['llm_same']} same ({llm_same_percent:.1f}%), {stats['llm_different']} different ({llm_different_percent:.1f}%)\"\n",
    "    )\n",
    "\n",
    "# Save the results to JSON\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_records, f, indent=4, ensure_ascii=False)\n",
    "print(f\"Saved comparison results to {output_json}\")\n",
    "\n",
    "# Save to Excel for easier viewing\n",
    "output_excel = output_json.replace(\".json\", \".xlsx\")\n",
    "pd.DataFrame(processed_records).to_excel(output_excel, index=False)\n",
    "print(f\"Saved comparison results to {output_excel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c6cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P04_official_reg_db_creation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
