{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5bca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = \"/home/gpinon/more_europa/clean_rdc_experiments/projects/P04_official_reg_db_creation\"\n",
    "os.chdir(working_dir)\n",
    "print(f\"Changed working directory to {working_dir}\")\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.p04_official_reg_db_creation import config\n",
    "import llm_backends\n",
    "from llm_backends.cache import DiskCacheStorage\n",
    "from llm_backends.mistral import dummy_config\n",
    "from llm_backends.openai import dummy_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TYPE =\"eval\" # \"test\" # \n",
    "RAW_PUBLICATIONS_DICT = {\n",
    "    \"eval\": \"../../datasets/001_publications_dataset/publications_dataset.jsonl\",\n",
    "    \"test\": \"../../datasets/001_publications_dataset/prod_publication_test_dataset.jsonl\",\n",
    "}\n",
    "FIELD = \"registry_name\"\n",
    "MODEL = \"small_mistral\"\n",
    "\n",
    "# Load environment variables from .env file and get API key\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_results=f\"data/from_notebooks/NW01/R02_comparison/{MODEL}/{FIELD}/compare_{FIELD}.json\"\n",
    "output_json = f\"data/from_notebooks/NW01/R03_evaluation/{MODEL}/{FIELD}/perf_metrics.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c42f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_percentage(value):\n",
    "    \"\"\"Format a float as percentage with 1 decimal place.\"\"\"\n",
    "    return f\"{value * 100:.1f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "out_dir = Path(output_json).parent\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load extraction results\n",
    "with open(extraction_results, \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(results)} records from {extraction_results}\")\n",
    "print(f\"Computing metrics for field '{FIELD}' and model '{MODEL}'\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_samples = len(df)\n",
    "correct_extractions = int(df[\"final_label\"].sum())\n",
    "incorrect_extractions = int(total_samples - correct_extractions)\n",
    "accuracy = correct_extractions / total_samples if total_samples > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c993a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by labeling_reason and final_label\n",
    "reason_groups = (\n",
    "    df.groupby([\"labeling_reason\", \"final_label\"]).size().reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Structure the breakdown data\n",
    "correct_reasons = {}\n",
    "incorrect_reasons = {}\n",
    "\n",
    "for _, row in reason_groups.iterrows():\n",
    "    reason = row[\"labeling_reason\"]\n",
    "    is_correct = row[\"final_label\"] == 1\n",
    "    count = int(row[\"count\"])\n",
    "\n",
    "    if is_correct:\n",
    "        correct_reasons[reason] = count\n",
    "    else:\n",
    "        incorrect_reasons[reason] = count\n",
    "\n",
    "# Create a structured breakdown\n",
    "reason_breakdown = {\n",
    "    \"correct_extractions\": {\n",
    "        \"total\": correct_extractions,\n",
    "        \"percentage\": accuracy,\n",
    "        \"reasons\": correct_reasons,\n",
    "    },\n",
    "    \"incorrect_extractions\": {\n",
    "        \"total\": incorrect_extractions,\n",
    "        \"percentage\": 1 - accuracy,\n",
    "        \"reasons\": incorrect_reasons,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create the summary structure\n",
    "summary = {\n",
    "    \"total_samples\": total_samples,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"correct_extractions\": correct_extractions,\n",
    "    \"incorrect_extractions\": incorrect_extractions,\n",
    "}\n",
    "\n",
    "# Collect all metrics into a single dictionary\n",
    "all_metrics = {\n",
    "    \"field\": FIELD,\n",
    "    \"model\": MODEL,\n",
    "    \"summary\": summary,\n",
    "    \"reason_breakdown\": reason_breakdown,\n",
    "}\n",
    "\n",
    "# Display summary metrics in logs with the requested format\n",
    "print(f\"Overall performance metrics:\")\n",
    "print(f\"  Total samples: {summary['total_samples']}\")\n",
    "print(\n",
    "    f\"  Accuracy: {format_percentage(accuracy)} ({correct_extractions} / {total_samples})\"\n",
    ")\n",
    "\n",
    "# Display correct extractions breakdown\n",
    "print(f\"  Correct extractions: {correct_extractions} ({format_percentage(accuracy)})\")\n",
    "for reason, count in correct_reasons.items():\n",
    "    percentage = count / total_samples if total_samples > 0 else 0\n",
    "    print(f\"        {reason}: {count} ({format_percentage(percentage)})\")\n",
    "\n",
    "# Display incorrect extractions breakdown\n",
    "print(\n",
    "    f\"  Incorrect extractions: {incorrect_extractions} ({format_percentage(1 - accuracy)})\"\n",
    ")\n",
    "for reason, count in incorrect_reasons.items():\n",
    "    percentage = count / total_samples if total_samples > 0 else 0\n",
    "    print(f\"        {reason}: {count} ({format_percentage(percentage)})\")\n",
    "\n",
    "# Save metrics to JSON\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_metrics, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved performance metrics to {output_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P04_official_reg_db_creation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
