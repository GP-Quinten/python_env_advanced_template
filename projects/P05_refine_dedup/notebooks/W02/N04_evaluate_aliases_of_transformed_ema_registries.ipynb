{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67690fe6",
   "metadata": {},
   "source": [
    "# Evaluate Aliases of Transformed EMA Registries\n",
    "\n",
    "This notebook prepares and explores alias pairs for LLM assessment, focusing on EMA registries that have been assigned to clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37425890",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5486884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set working directory (adjust if needed)\n",
    "working_dir = '/home/gpinon/more_europa/clean_rdc_experiments/projects/P05_refine_dedup'\n",
    "os.chdir(working_dir)\n",
    "print(f'Changed working directory to {working_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4869348",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00c331a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EMA prediction results: (237, 10)\n",
      "Loaded clusters table: (54335, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load EMA prediction results (output of R03)\n",
    "ema_pred_xlsx = 'data/W02/R03_evaluate_model_performance_on_ema_registries/ema_prediction_results.xlsx'\n",
    "ema_pred_df = pd.read_excel(ema_pred_xlsx)\n",
    "print(f'Loaded EMA prediction results: {ema_pred_df.shape}')\n",
    "\n",
    "# Load clusters table\n",
    "clusters_table_xlsx = 'data/W02/R02_evaluate_model_performance/clusters_table.xlsx'\n",
    "clusters_df = pd.read_excel(clusters_table_xlsx)\n",
    "print(f'Loaded clusters table: {clusters_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b16d11",
   "metadata": {},
   "source": [
    "## 3. Prepare Alias Pairs for LLM Assessment\n",
    "For each EMA registry that landed in a cluster, collect all aliases in the same cluster (≤50/all, >50/top 50 by frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739e5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformed EMA registries: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 120.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 72 EMA-alias pairs for LLM assessment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Merge clusters info for alias lookup\n",
    "# clusters_df['Final_Cluster'] = clusters_df['Final_Cluster'].astype(str)\n",
    "# # Only consider EMA registries that are not noise\n",
    "# def is_noise(cluster):\n",
    "#     return str(cluster) == '0' or str(cluster).endswith('_0')\n",
    "# ema_transformed_df = ema_pred_df[~ema_pred_df['assigned_cluster'].apply(is_noise)].copy()\n",
    "# print(f'Number of transformed EMA registries: {ema_transformed_df.shape[0]}')\n",
    "# # Prepare alias pairs\n",
    "# alias_pairs = []\n",
    "# max= 1  # Maximum number of aliases per EMA registry\n",
    "# for idx, row in tqdm(ema_transformed_df.iterrows(), total=ema_transformed_df.shape[0]):\n",
    "#     ema_name = row['ema_full_name']\n",
    "#     ema_object_id = row.get('ema_object_id', None)\n",
    "#     cluster_id = row['assigned_cluster']\n",
    "#     # Get all aliases in the same cluster\n",
    "#     aliases = clusters_df[clusters_df['Final_Cluster'] == cluster_id]\n",
    "#     # Exclude the EMA registry itself if present\n",
    "#     aliases = aliases[aliases['full_name'] != ema_name]\n",
    "#     # Sort aliases by number_of_occurrences\n",
    "#     aliases = aliases.sort_values('number_of_occurrences', ascending=False)\n",
    "#     # If cluster size > 50, take top 50\n",
    "#     if len(aliases) > max:\n",
    "#         aliases = aliases.head(max)\n",
    "#     for _, alias_row in aliases.iterrows():\n",
    "#         alias_pairs.append({\n",
    "#             'ema_full_name': ema_name,\n",
    "#             'assigned_cluster': cluster_id,\n",
    "#             'alias': alias_row['full_name'],\n",
    "#             'alias_nb_occ': alias_row['number_of_occurrences'],\n",
    "#             'ema_object_id': ema_object_id\n",
    "#         })\n",
    "# alias_pairs_df = pd.DataFrame(alias_pairs)\n",
    "# print(f'Prepared {alias_pairs_df.shape[0]} EMA-alias pairs for LLM assessment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aac072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformed EMA registries: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 1303.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 70 EMA-alias pairs for LLM assessment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge clusters info for alias lookup\n",
    "clusters_df['Final_Cluster'] = clusters_df['Final_Cluster'].astype(str)\n",
    "# Only consider EMA registries that are not noise\n",
    "def is_noise(cluster):\n",
    "    return str(cluster) == '0' or str(cluster).endswith('_0')\n",
    "ema_transformed_df = ema_pred_df[ema_pred_df['assigned_cluster'].apply(is_noise)].copy()\n",
    "print(f'Number of transformed EMA registries: {ema_transformed_df.shape[0]}')\n",
    "# Prepare alias pairs\n",
    "alias_pairs = []\n",
    "max= 1  # Maximum number of aliases per EMA registry\n",
    "for idx, row in tqdm(ema_transformed_df.iterrows(), total=ema_transformed_df.shape[0]):\n",
    "    ema_name = row['ema_full_name']\n",
    "    ema_object_id = row.get('ema_object_id', None)\n",
    "    cluster_id = row['assigned_cluster']\n",
    "    # # Get all aliases in the same cluster\n",
    "    aliases = ema_transformed_df[ema_transformed_df['ema_full_name'] == ema_name]\n",
    "    # Exclude the EMA registry itself if present\n",
    "    aliases = aliases[aliases['closest'] != ema_name]\n",
    "    # # Sort aliases by number_of_occurrences\n",
    "    # aliases = aliases.sort_values('number_of_occurrences', ascending=False)\n",
    "    # If cluster size > 50, take top 50\n",
    "    if len(aliases) > max:\n",
    "        aliases = aliases.head(max)\n",
    "    for _, alias_row in aliases.iterrows():\n",
    "        alias_pairs.append({\n",
    "            'ema_full_name': ema_name,\n",
    "            'assigned_cluster': cluster_id,\n",
    "            'alias': alias_row['closest'],\n",
    "            'alias_nb_occ': alias_row['closest_nb_occ'],\n",
    "            'ema_object_id': ema_object_id\n",
    "        })\n",
    "alias_pairs_df = pd.DataFrame(alias_pairs)\n",
    "print(f'Prepared {alias_pairs_df.shape[0]} EMA-alias pairs for LLM assessment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af764991",
   "metadata": {},
   "source": [
    "## 4. LLM Judgement: Prepare Prompts and Run Inference\n",
    "Load the model config and prompt, prepare the prompts, make inferences on all prepared pairs, and parse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01e25fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # slected first 5 for testing\n",
    "# alias_pairs_df = alias_pairs_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfadf79",
   "metadata": {},
   "source": [
    "## Prepare prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcc80e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 70 prompts for LLM assessment.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load prompt template\n",
    "prompt_txt = 'etc/prompts/prompt_compare_registry_names.txt'\n",
    "with open(prompt_txt, 'r') as pf:\n",
    "    base_prompt = pf.read().strip()\n",
    "\n",
    "# Load model config\n",
    "model_config_path = 'etc/configs/gpt4_1_openai_config.json'  # adjust if needed\n",
    "with open(model_config_path, 'r', encoding='utf-8') as f:\n",
    "    model_cfg = json.load(f)\n",
    "\n",
    "# Prepare prompts\n",
    "def construct_prompt(base_prompt: str, name1: str, name2: str) -> str:\n",
    "    return base_prompt.replace('{{content_a}}', name1).replace('{{content_b}}', name2)\n",
    "\n",
    "prompts = []\n",
    "for idx, row in alias_pairs_df.iterrows():\n",
    "    prompt = construct_prompt(base_prompt, row['ema_full_name'], row['alias'])\n",
    "    prompts.append({\n",
    "        'prompt': prompt,\n",
    "        'custom_id': f\"{row['ema_full_name']}|||{row['alias']}\",\n",
    "        'ema_full_name': row['ema_full_name'],\n",
    "        'alias': row['alias'],\n",
    "        'assigned_cluster': row['assigned_cluster'],\n",
    "        'alias_nb_occ': row['alias_nb_occ'],\n",
    "        'ema_object_id': row.get('ema_object_id', None)\n",
    "    })\n",
    "print(f'Prepared {len(prompts)} prompts for LLM assessment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60ea62",
   "metadata": {},
   "source": [
    "## Make inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e72c839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "411e6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM backend setup (same as in S03_eval_pairs_similarity_assessment_with_llm)\n",
    "import llm_backends\n",
    "from llm_inference.cache.tmp import TmpCacheStorage\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "cache_storage = TmpCacheStorage()\n",
    "backend = llm_backends.OpenAIAsyncBackend(api_key=os.getenv('OPENAI_API_KEY'), cache_storage=cache_storage)\n",
    "\n",
    "async def run_async_inference(prompts, backend, model_cfg):\n",
    "    raw_responses = []\n",
    "    pbar = tqdm(total=len(prompts), desc='LLM Inference')\n",
    "    for prompt in prompts:\n",
    "        raw_response = await backend.infer_one(prompt, model_cfg)\n",
    "        raw_response['custom_id'] = prompt['custom_id']\n",
    "        raw_responses.append(raw_response)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return raw_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33b02f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 10:02:57,773 - root - WARNING - Starting asynchronous inference ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference: 100%|██████████| 70/70 [01:31<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM inference complete. 70 responses collected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logging.warning(\"Starting asynchronous inference ...\")\n",
    "loop = asyncio.get_event_loop()\n",
    "raw_responses = await run_async_inference(prompts, backend, model_cfg)\n",
    "print(f'LLM inference complete. {len(raw_responses)} responses collected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95990b",
   "metadata": {},
   "source": [
    "## Parse and collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bca13667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 70 LLM responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Parse and collect results\n",
    "prompt_map = {p['custom_id']: p for p in prompts}\n",
    "llm_responses = []\n",
    "for raw_response in tqdm(raw_responses, desc='Processing responses', leave=False):\n",
    "    custom_id = raw_response.get('custom_id', '')\n",
    "    prompt_obj = prompt_map.get(custom_id)\n",
    "    if prompt_obj:\n",
    "        parsed_response = backend._parse_response(raw_response)\n",
    "        parsed_response['custom_id'] = custom_id\n",
    "        parsed_response['ema_full_name'] = prompt_obj['ema_full_name']\n",
    "        parsed_response['alias'] = prompt_obj['alias']\n",
    "        parsed_response['assigned_cluster'] = prompt_obj['assigned_cluster']\n",
    "        parsed_response['alias_nb_occ'] = prompt_obj['alias_nb_occ']\n",
    "        parsed_response['ema_object_id'] = prompt_obj['ema_object_id']\n",
    "        llm_responses.append(parsed_response)\n",
    "llm_responses_df = pd.DataFrame(llm_responses)\n",
    "print(f'Parsed {llm_responses_df.shape[0]} LLM responses.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28128cab",
   "metadata": {},
   "source": [
    "# 5. Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "302609a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LLM responses into alias_pairs_df\n",
    "merged_df = alias_pairs_df.merge(\n",
    "    llm_responses_df,\n",
    "    on=[\"ema_full_name\", \"alias\", \"assigned_cluster\", \"alias_nb_occ\", \"ema_object_id\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "\n",
    "# Assign final_label: 1 if LLM says 'same', 0 if 'different', else uncertain\n",
    "def get_final_label(row):\n",
    "    if \"final_decision\" in row and row[\"final_decision\"] == \"same\":\n",
    "        return 1\n",
    "    elif \"final_decision\" in row and row[\"final_decision\"] == \"different\":\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "\n",
    "merged_df[\"final_label\"] = merged_df.apply(get_final_label, axis=1)\n",
    "# Sort as required: by cluster popularity, then alias popularity\n",
    "cluster_sizes = merged_df.groupby(\"assigned_cluster\").size().to_dict()\n",
    "merged_df[\"cluster_popularity\"] = merged_df[\"assigned_cluster\"].map(cluster_sizes)\n",
    "merged_df = merged_df.sort_values(\n",
    "    [\"cluster_popularity\", \"alias_nb_occ\"], ascending=[False, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97e0b3",
   "metadata": {},
   "source": [
    "# 6. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8377fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM-assessed alias pairs to data/W02/R04_evaluate_aliases_of_transformed_ema_registries/assessed_aliases_noise.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save to Excel\n",
    "output_xlsx = (\n",
    "    \"data/W02/R04_evaluate_aliases_of_transformed_ema_registries/assessed_aliases_noise.xlsx\"\n",
    ")\n",
    "output_dir = Path(output_xlsx).parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "merged_df.to_excel(output_xlsx, index=False)\n",
    "print(f\"Saved LLM-assessed alias pairs to {output_xlsx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd71ab2",
   "metadata": {},
   "source": [
    "# 7. Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e4e9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "output_xlsx = (\n",
    "    \"data/W02/R04_evaluate_aliases_of_transformed_ema_registries/assessed_aliases_noise.xlsx\"\n",
    ")\n",
    "assessed_aliases_df = pd.read_excel(output_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8833227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformed EMA registries with at least 1 good alias: 4\n",
      "Number of transformed EMA registries with at least 2 good aliases: 0\n",
      "Number of transformed EMA registries with at least 3 good aliases: 0\n",
      "Number of transformed EMA registries with at least 4 good aliases: 0\n",
      "Number of transformed EMA registries with at least 5 good aliases: 0\n"
     ]
    }
   ],
   "source": [
    "# Analysis: EMA registries with at least 1 or 2 'good' aliases\n",
    "ema_good_alias_counts = (\n",
    "    assessed_aliases_df[assessed_aliases_df[\"final_label\"] == 1]\n",
    "    .groupby(\"ema_full_name\")\n",
    "    .size()\n",
    ")\n",
    "for threshold in range(1, 6):\n",
    "    count = (ema_good_alias_counts >= threshold).sum()\n",
    "    alias_label = \"alias\" if threshold == 1 else \"aliases\"\n",
    "    print(\n",
    "        f\"Number of transformed EMA registries with at least {threshold} good {alias_label}: {count}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3525d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of good aliases per transformed EMA registry: 1.00\n",
      "Mean number of wrong aliases per transformed EMA registry: 1.00\n"
     ]
    }
   ],
   "source": [
    "# mean number of good aliases per EMA registry\n",
    "mean_good_aliases = ema_good_alias_counts.mean()\n",
    "print(f\"Mean number of good aliases per transformed EMA registry: {mean_good_aliases:.2f}\")\n",
    "# mean number of wrong aliases per EMA registry\n",
    "mean_wrong_aliases = (\n",
    "    assessed_aliases_df[assessed_aliases_df[\"final_label\"] == 0]\n",
    "    .groupby(\"ema_full_name\")\n",
    "    .size()\n",
    "    .mean()\n",
    ")\n",
    "print(f\"Mean number of wrong aliases per transformed EMA registry: {mean_wrong_aliases:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "822e345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ratio of good aliases per transformed EMA registry: 5.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29194/1533864510.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_good_alias_ratio)\n"
     ]
    }
   ],
   "source": [
    "# mean % of good aliases (vs wrong aliases) per EMA registry\n",
    "# for each EMA registry, calculate the ratio of good to total aliases\n",
    "def calculate_good_alias_ratio(group):\n",
    "    good_count = group[group[\"final_label\"] == 1].shape[0]\n",
    "    total_count = group.shape[0]\n",
    "    return good_count / total_count if total_count > 0 else 0\n",
    "ema_good_alias_ratios = (\n",
    "    assessed_aliases_df.groupby(\"ema_full_name\")\n",
    "    .apply(calculate_good_alias_ratio)\n",
    ")\n",
    "mean_good_alias_ratio = ema_good_alias_ratios.mean()\n",
    "print(f\"Mean ratio of good aliases per transformed EMA registry: {mean_good_alias_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "663e53ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ratio of good aliases in large clusters: 3.77%\n",
      "Mean ratio of good aliases in small clusters: 11.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29194/4016142208.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_good_alias_ratio)\n",
      "/tmp/ipykernel_29194/4016142208.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_good_alias_ratio)\n"
     ]
    }
   ],
   "source": [
    "# compute the same for but subsets of data\n",
    "# first for EMA_registries that are in clusters with more than 50 aliases\n",
    "n = 10\n",
    "large_clusters_df = assessed_aliases_df[\n",
    "    assessed_aliases_df[\"assigned_cluster\"].isin(\n",
    "        assessed_aliases_df[\"assigned_cluster\"].value_counts()[lambda x: x > n].index\n",
    "    )\n",
    "]\n",
    "# then for EMA_registries that are in clusters with less than 50 aliases\n",
    "small_clusters_df = assessed_aliases_df[\n",
    "    assessed_aliases_df[\"assigned_cluster\"].isin(\n",
    "        assessed_aliases_df[\"assigned_cluster\"].value_counts()[lambda x: x <= n].index\n",
    "    )\n",
    "]\n",
    "\n",
    "# compute Mean ratio of good aliases per transformed EMA registry for large clusters\n",
    "large_clusters_good_alias_ratios = (\n",
    "    large_clusters_df.groupby(\"ema_full_name\")\n",
    "    .apply(calculate_good_alias_ratio)\n",
    ")\n",
    "mean_large_clusters_good_alias_ratio = large_clusters_good_alias_ratios.mean()\n",
    "print(f\"Mean ratio of good aliases in large clusters: {mean_large_clusters_good_alias_ratio:.2%}\")\n",
    "\n",
    "# compute Mean ratio of good aliases per transformed EMA registry for small clusters\n",
    "small_clusters_good_alias_ratios = (\n",
    "    small_clusters_df.groupby(\"ema_full_name\")\n",
    "    .apply(calculate_good_alias_ratio)\n",
    ")\n",
    "mean_small_clusters_good_alias_ratio = small_clusters_good_alias_ratios.mean()\n",
    "print(f\"Mean ratio of good aliases in small clusters: {mean_small_clusters_good_alias_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1167f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P05_refine_dedup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
